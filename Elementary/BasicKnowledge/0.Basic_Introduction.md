

# RL Introduction

## 和其他 ML 领域的异同：

### 数据和标签

1. 数据并非 i.i.d : .: previous outputs influence future inputs!
1. 没有 Groud Truth Answer ： 只有目标达成与否 （或者是 reward）

### Deep RL ？
1. RL ： 本身提供了 form for behavior
   Deep ： 只是一种处理 **Unstructured Data （Environment）** 的工具
2. Deep RL ：有助于得到 end-to-end learning for sequential decision making
   - 尤其是有助于 robotics 等复杂问题



## RL 相关的 ML Topic

1. RL ： maximize rewards
2. Inverse RL ： learning rewards function from examples
2. Imitation learning ： learning from demo
2. Transfer , Meta Learning : Learning from other tasks

## Formalize RL

0. RL formalism includes:
   1. Environment : dynamic of problem
   2. Reward : goal
   3. Agents :
      - agent state
      - policy
      - value function
      - model

### Environment

0. At each step $t$ 
   1. **The agent:**
      1. Receives observation $O_{t}$ (and reward $R_{t}$ )
      2. Executes action $A_{t}$
   2. **The environment:**
     1. Receives action $A_{t}$
     2. Emits observation $O_{t+1}$ (and reward $R_{t+1}$ )

### Reward Signal

0. <u>Any goal can be formalized as the outcome of maximizing a cumulative reward</u>
   1. Maximize: $G_{t}=R_{t+1}+R_{t+2}+R_{t+3}+\ldots$


### Values
1. **Value**: Expected Cumulative Reward <u>from state s</u>: 
   1. $\begin{aligned} v(s) &=\mathbb{E}\left[G_{t} \mid S_{t}=s\right] \\ &=\mathbb{E}\left[R_{t+1}+R_{t+2}+R_{t+3}+\ldots \mid S_{t}=s\right] \end{aligned}$
   2. Also defined recursively : $v(s)=\mathbb{E}\left[R_{t+1}+v\left(S_{t+1}\right) \mid S_{t}=s\right]$

2. **Action Value**: Expected Cumulative Reward from state s condition on action a:
   1. $\begin{aligned} q(s, a) &=\mathbb{E}\left[G_{t} \mid S_{t}=s, A_{t}=a\right] \\ &=\mathbb{E}\left[R_{t+1}+R_{t+2}+R_{t+3}+\ldots \mid S_{t}=s, A_{t}=a\right] \end{aligned}$



### Agents
#### Agent State
1. **Environment State** : environment's internal state + invisible to the agent
2. **Fully Observable Environment**: agent could see the full environment state
   1. $S_{t}=O_{t}=$ environment state, where $O_t$ is the observation of agent

3. **Partially Observable Environment**
4. **Agent State**: a function of the history, many possible functions
   1. fully observable: $S_{t}=O_{t}$
   2. more generally: 
      1. History: $\mathcal{H}_{t}=O_{0}, A_{0}, R_{1}, O_{1}, \ldots, O_{t-1}, A_{t-1}, R_{t}, O_{t}$
      2. State should be constructed based on history $S_{t+1}=u\left(\mathcal{H}_{t}, O_{t+1}\right)$

   3. **Markov decision process** : 
      1. Markov: $p\left(r, s \mid S_{t}, A_{t}\right)=p\left(r, s \mid \mathcal{H}_{t}, A_{t}\right)$
      2. <u>Once the state is known, the history may be thrown away</u>
      3. **Agent state under MDP: ** $S_{t+1}=u\left(S_{t}, A_{t}, R_{t+1}, O_{t+1}\right)$


#### Policy
0. Policy: a map from agent state to action, 2 different types:
   1. Deterministic policy:  $A=\pi(S)$
   2. Stochastic policy : $\pi(A \mid S)=p(A \mid S)$


1. **Trade-off:** 
   1. **off-policy**: improve the <u>policy(may be beased on q function)</u>, without generating new samples ==from this policy==
   2. **on-policy**: each time policy is changed, we need to generate new samples to update policy




#### Value Function
0. **Value Function Definition**: expected cumulated reward (with discount factor)
   1. $\begin{aligned} v_{\pi}(s) &=\mathbb{E}\left[G_{t} \mid S_{t}=s, \pi\right] \\ &=\mathbb{E}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots \mid S_{t}=s, \pi\right] \end{aligned}$
   2. **discountor factor: ** Trades off importance of immediate vs long-term rewards

1. **Value Function Recursive Definition and Bellman Equation**
   1. Bellman Equation ( depend on policy ) : $\begin{aligned} v_{\pi}(s) &=\mathbb{E}\left[R_{t+1}+\gamma G_{t+1} \mid S_{t}=s, A_{t} \sim \pi(s)\right] \\ &=\mathbb{E}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s, A_{t} \sim \pi(s)\right] \end{aligned}$
   2. Bellman Optimal Equation (not depend on policy ):  $v_{*}(s)=\max _{a} \mathbb{E}\left[R_{t+1}+\gamma v_{*}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=a\right]$

2. **Q-Function**: total reward from taking $a_t$ in $s_t$:
   1. $Q^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=\sum_{t^{\prime}=t}^{T} E_{\pi_{\theta}}\left[r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right) \mid \mathbf{s}_{t}, \mathbf{a}_{t}\right]$
   2. Relation with value function: $V^{\pi}\left(\mathbf{s}_{t}\right)=E_{\mathbf{a}_{t} \sim \pi\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\left[Q^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]$

3. **Trade-off**:
   1. **Value-function fitting:** sample efficient like value iteration but <u>not guaranteed to converge ot anything</u>


#### Model
0. Model: predict what env will do next, predict many things: 
   1. Next state:  $\mathcal{P}\left(s, a, s^{\prime}\right) \approx p\left(S_{t+1}=s^{\prime} \mid S_{t}=s, A_{t}=a\right)$
   2. Next reward: $\mathcal{R}(s, a) \approx \mathbb{E}\left[R_{t+1} \mid S_{t}=s, A_{t}=a\right]$

