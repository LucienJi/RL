#  Model Free Value Based

0. Recap:
   1. Task: 
      1. **Prediction** : Estimate the performance of policy
      2. **Control** : Find the optimal policy
   2. Methods:
      1. **Dynamic Programming**: for a known MDP
      2. **Monte Carlo** Methods: unknown MDP
      3. **Temporal Difference** : unknow MDP

<img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-26 at 9.52.08 AM.png" alt="Screen Shot 2022-01-26 at 9.52.08 AM" style="zoom:25%;" /> <img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-26 at 9.52.24 AM.png" alt="Screen Shot 2022-01-26 at 9.52.24 AM" style="zoom:25%;" /> <img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-26 at 9.52.43 AM.png" alt="Screen Shot 2022-01-26 at 9.52.43 AM" style="zoom:25%;" />



## Monte Carlo

0. Why we use MC ?
   1. Use samples to learn without a model
   2. Model-free: no MDP required, only use samples
1. For example, for the problem multi-armed bandit : $q_{t}(a)=\frac{\sum_{i=0}^{t} \mathcal{I}\left(A_{i}=a\right) R_{i+1}}{\sum_{i=0}^{t} \mathcal{I}\left(A_{i}=a\right)} \approx \mathbb{E}\left[R_{t+1} \mid A_{t}=a\right]=q(a)$ 

### Prediction and Evaluation
1. **Goal** : learn $v_{\pi}$ from episodes of experience under policy $\pi$ 
   1. Look at the return, total discounted reward : $G_{t}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{T-t-1} R_{T}$ 
   2. value function is the expected  return : $v_{\pi}(s)=\mathbb{E}\left[G_{t} \mid S_{t}=s, \pi\right]$
   3. **Monte Carlo** : sample average return instead of expected

2. **Disadvantages: **
   1. **If episodes are long, learning can be slow**, we have to wait until an episode ends
   2. **High variance** for the return.



### Control

1. **Generalized Policy Iteration** with Action-Value Function 
   1. **Policy Evaluation**:  Monte-Carlo evaluation 
   2. **Policy Improvement: ** $\epsilon-greedy$ policy improvement 

#### GLIE
1. **GLIE** : greedy in the limit with infinite exploration.
   1. all state - action paris are explored infinitely many times.
   2. policy converge to a greedy policy

2.  Model-Free Contro under GLIE Repeat :
   1. Sample episode k using policy $\sim \pi $
   2. For each state $s_t$ and $a_t$ , update the $q(s_t,a_t) \leftarrow q(s_t,a_t) + \alpha_t (G_t - q(s_t,a_t))$
      1. where $\alpha_t = \frac{1}{N(s_t,a_t)}$ 

   3. Improve policy: 
      1. $\epsilon \leftarrow 1/k$
      2. $\pi \leftarrow \epsilon -greedy$


2. **Theorem: ** GLIE converges to the optimal action-value 


## Tempoal Difference

0. Why we use TD ?
   1. there are 2 ways to represent value functions : 
      1. **Expected return** : use MC to sample it
      2. **Bellman expected equation**  : use TD to sample it
1. TD learning by sampling Bellman Equations
   1. Bellman Equation: $v_{\pi}(s)=\mathbb{E}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s, A_{t} \sim \pi\left(S_{t}\right)\right]$
   2. Approximate by iterating : $v_{k+1}(s)=\mathbb{E}\left[R_{t+1}+\gamma v_{k}\left(S_{t+1}\right) \mid S_{t}=s, A_{t} \sim \pi\left(S_{t}\right)\right]$
      1. Sample : $v_{t+1}\left(S_{t}\right) \sim R_{t+1}+\gamma v_{t}\left(S_{t+1}\right)$  

### Prediction and Evaluation

1. **Goal** ： learn $v_{\pi}(s)$ or  $q_{\pi}(s,a)$  online from experience under policy $\pi$
2. For **value function**: Update value towards estimated return : $\boldsymbol{R}_{t+1}+\gamma v\left(S_{t+1}\right)$ 
   1. $v_{t+1}\left(S_{t}\right) \leftarrow v_{t}\left(S_{t}\right)+\alpha(\overbrace{\underbrace{R_{t+1}+\gamma v_{t}\left(S_{t+1}\right)}_{\text {target }}-v_{t}\left(S_{t}\right)}^{\text {TD error }})$ 
   2. TD error : try to minimize the td-error : $\delta_{t}=R_{t+1}+\gamma v_{t}\left(S_{t+1}\right)-v_{t}\left(S_{t}\right)$

#### SARSA 

1. For **action values**: **SARSA**
   1. Update value $q_{t}\left(S_{t}, A_{t}\right)$ towards estimated return $\boldsymbol{R}_{\boldsymbol{t}+1}+\gamma q\left(S_{\boldsymbol{t}+1}, \boldsymbol{A}_{\boldsymbol{t}+1}\right)$
   2. $q_{t+1}\left(S_{t}, A_{t}\right) \leftarrow q_{t}\left(S_{t}, A_{t}\right)+\alpha(\underbrace{\overbrace{\boldsymbol{R _ { t + 1 } + \gamma q _ { t } ( S _ { t + 1 } , A _ { t + 1 } )}}_{\text {TD error }}-q_{t}\left(S_{t}, A_{t}\right)}^{\text {Target }})$



### Control
#### Tabular SARSA
1. **Policy Evaluation** : target error :$r + \gamma Q(s^{\prime},a^{\prime}) - Q(s,a) $ 
1. **Policy Improvement** : $\epsilon - greedy$ policy improvement
1. **Theorem** : Tabular SARSA converges to the optimal action-value function 
#### Q-Learning

0. Source of Algorithm : from dynamic programming to sampling , we use only the policy iteration. Why we can not use the same idea for the value iteration ? 
1. **Q-learning** : 
   1. **Analogy to value iteration**
   2. **Off-policy ** : 
      1. Learn about **target** policy from experience sampled from other policy
      2. Same Trajectory could be trained for different goals.
      3. Learn from observing human or other agents
      4. Learn about **multiple** or **greedy** policy while following **one** or **exploratory** policy.
   3. **Q-Learning** estimate the value of the **greedy** policy
      1. $q_{t+1}(s,a) = q_{t}(s,a) + \alpha_{t} (r + \gamma \text{max}_{a^{\prime}} q_{t}(s^{\prime},a^{\prime}) - q_{t}(s,a))$ 
      2. Theorem : converge to optimal action-value function if we take each action in each state inifinitely often. 
2. Summary : 
   1. Work for any policy as long as it selects all actions sufficiently often
   2. just need decaying step sizes: infinite sum and finite L2 sum.

#### Double Q-learning

1. Problem of Q-learning: 
   1. Use same values to select and to evaluate
   2. **more** likely to select **overestimated** values 
2. **Double Q-learning**
   1. Store two action-value functions : Use A to evaluate the B's choice  in order to make 
   2. New target value : 
      1. $R_{t+1}+\gamma q_{t}^{\prime}\left(S_{t+1}, \underset{a}{\operatorname{argmax}} q_{t}\left(S_{t+1}, a\right)\right)$ 
      2. $R_{t+1}+\gamma q_{t}\left(S_{t+1}, \underset{a}{\operatorname{argmax}} q_{t}^{\prime}\left(S_{t+1}, a\right)\right)$ 
   3. **Double Learning**
      1. Could be generalized to other updates : eg SARSA use the greedy, so may be also overestimated, so we could use double SARSA. 

#### Expected SARSA

1. Importance Sampling for **Off-Policy TD updates** : weighted TD target :
   1. $v\left(S_{t}\right) \leftarrow v\left(S_{t}\right)+\alpha\left(\frac{\pi\left(A_{t} \mid S_{t}\right)}{\mu\left(A_{t} \mid S_{t}\right)}\left(R_{t+1}+\gamma v\left(S_{t+1}\right)\right)-v\left(S_{t}\right)\right)$  
2. Expected SARSA : method of policy evaluation for $\pi$ and exploration could use the buffer from other policy.
   1. $q\left(S_{t}, A_{t}\right) \leftarrow q\left(S_{t}, A_{t}\right)+\alpha\left(\boldsymbol{R}_{t+1}+\gamma \sum_{a} \pi\left(a \mid S_{t+1}\right) q\left(S_{t+1}, a\right)-q\left(S_{t}, A_{t}\right)\right)$ 
3. SARSA is special case for expected SARSA when policy is greedy. 




## Between MC and TD: Multi-Step TD

<img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-26 at 10.18.10 AM.png" alt="Screen Shot 2022-01-26 at 10.18.10 AM" style="zoom:50%;" />

0. Limits of TD and MC
   1. TD's **estimate may be inaccurate**
   2. TD's **Information can propagate back very slowly**
   3. MC's propagation is fast by **nosiy**. 

### Prediction and Evaluation

1. **Multi-step Return （n-step）**:
   1. n=1 (TD) : $G_{t}^{(1)}=R_{t+1}+\gamma v\left(S_{t+1}\right)$
   2. n=Infinite (MC) : $G_{t}^{(\infty)}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{T-t-1} R_{T}$
   3. n-step :  $G_{t}^{(n)}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{n-1} R_{t+n}+\gamma^{n} v\left(S_{t+n}\right)$ 
2. **Mixing Multi-step Returns ( $\lambda - Return$ )** : <u>bootstrapping + multi-step return</u>
   1. $G_{t}^{(n)}=R_{t+1}+\gamma G_{t+1}^{(n-1)}$ .  Not the original norm, it's the multi-step return
   2. Bootstrap : $G_{t}^{(1)}=R_{t+1}+\gamma v\left(S_{t+1}\right)$
   3. Bootstrap a bit : $G_{t}^{\lambda}=R_{t+1}+\gamma\left((1-\lambda) v\left(S_{t+1}\right)+\lambda G_{t+1}^{\lambda}\right)$ 
   4. **Weighted average n-step returns: ** $G_{t}^{\lambda}=\sum_{n=1}^{\infty}(1-\lambda) \lambda^{n-1} G_{t}^{(n)}$ 
   5. Special Case:
      1. TD : $G_{t}^{\lambda=0}=R_{t+1}+\gamma v\left(S_{t+1}\right)$ 
      2. MC : $G_{t}^{\lambda=1}=R_{t+1}+\gamma G_{t+1}$ 
   6. Advantage & Limits
      1. bootstrap has issue with bias; monte carlo has issue with variance; **multi-step returns** and lambda-return have benefits from both TD and MC.
      2. However : 
         1. mixing multi-step returns still have to wait for the end of episode. **Not independent of Span**.  
         2. Not efficient : all the computation is done at the end of episode -> **not online learning**

### Eligibility Traces : TD ($\lambda$)

0. How to become independent of span and computationally efficient ? 

   1. **n-step** or **mc **or $\lambda $ - return is **forward view**
   2. eligibility traces is **backward view** : keep track of history updates and assign the new target error according to the passing state. 

1. Implementation:

   1. First we propose the **eligibility trace vector** which has the same shape as the parameters in function approximation

   2. Incremented **eligibility trace vector** on each time step by the **value of gradient** : 
      $$
      \begin{aligned}&\mathbf{z}_{-1} \doteq \mathbf{0} \\&\mathbf{z}_{t} \doteq \gamma \lambda \mathbf{z}_{t-1}\end{aligned}+\nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right), \quad 0 \leq t \leq T
      $$

   3. Update the weight vector based on the **TD error** and the **vector eligibility trace**: 
      $$
      \delta_{t} \doteq R_{t+1}+\gamma \hat{v}\left(S_{t+1}, \mathbf{w}_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\\
      \mathbf{w}_{t+1} \doteq \mathbf{w}_{t}+\alpha \delta_{t} \mathbf{z}_{t}
      $$

   4. TD(1) is a general way to implement Monte Carlo methods, which is easier to apply and could be **on-line** 

   5. $\lambda$ - return and TD ($\lambda$) perform **virtually identically** when step size is small . 

   6.  **Real, True Implementation of Online TD($\lambda$)** : which has the same idea of $\lambda$ - return :

      1. For linear case : $\hat{v}(s, \mathbf{w})=\mathbf{w}^{\top} \mathbf{x}(s)$ 

      2. $$
         \mathbf{w}_{t+1} \doteq \mathbf{w}_{t}+\alpha \delta_{t} \mathbf{z}_{t}+\alpha\left(\mathbf{w}_{t}^{\top} \mathbf{x}_{t}-\mathbf{w}_{t-1}^{\top} \mathbf{x}_{t}\right)\left(\mathbf{z}_{t}-\mathbf{x}_{t}\right) \\
         \mathbf{z}_{t} \doteq \gamma \lambda \mathbf{z}_{t-1}+\left(1-\alpha \gamma \lambda \mathbf{z}_{t-1}^{\top} \mathbf{x}_{t}\right) \mathbf{x}_{t}
         $$

      3. Produce the same sequence of weight vectors as the online $\lambda$ - return. 

         <img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-27 at 8.52.20 AM.png" alt="Screen Shot 2022-01-27 at 8.52.20 AM" style="zoom:50%;" /> 

         

2. **Intuition for Eligibility Trace** the derivation is not the exact TD($\lambda$), but could be used for online MC  :

   1. First, we propose the funciton approximation : $v_{\theta} (s_t)$, we note the **TD error** : $\delta_{t}= R_{t} + \gamma v_{\theta}(s_{t+1}) - v_{\theta}(s_t)$  and we note the **MC error** : $G_{t} - v_{\theta}(s_t)$ . where if possible we could use $\lambda$ - return for $G_t$ 

   2. Second, $\Delta \theta = \sum_{t=0}^{T-1} \alpha\left(G_{t}-v\left(s_{t}\right)\right)\nabla v_{\theta}(s_t) $ , gradient for $v_{\theta}$ by minimizing the **MC error** at the end of episode. 

   3. Third, we rewrite the MC error at the end of episode. We recall that MC Error can only be calculated at the end. 
      $$
      \begin{aligned}
      G_{t}-v\left(S_{t}\right) &=R_{t}+\gamma G_{t+1}-v\left(S_{t}\right) \\
      &=\underbrace{R_{t}+\gamma v\left(S_{t+1}\right)-v\left(S_{t}\right)}_{=\delta_{t}}+\gamma\left(G_{t+1}-v\left(S_{t+1}\right)\right) \\
      &=\delta_{t}+\gamma\left(G_{t+1}-v\left(S_{t+1}\right)\right) \\
      &=\ldots \\
      &=\delta_{t}+\gamma \delta_{t+1}+\gamma^{2}\left(G_{t+2}-v\left(S_{t+2}\right)\right) \\
      &=\ldots \\
      &=\sum_{k=t}^{T} \gamma^{k-t} \delta_{k}
      \end{aligned}
      $$
      
   4. Then, we replace the **MC Error** in gradient with the **TD Error**. $\Delta \theta = \sum_{t=0}^{T-1} \alpha (\sum_{k=t}^{T} \gamma^{k-t} \delta_{k} )\nabla v_{\theta}(s_t) $ . We should also use the technique of change of variable by considering the triangle sum:  $\Delta \theta =  \sum_{k=0}^{T-1} \alpha \delta_{k} \sum_{t=0}^{k} \gamma^{k-t} \nabla v_{\theta}(s_t)$. Now we can pose the **eligibility trace vector : $e_k$** = $\sum_{t=0}^{k} \gamma^{k-t} \nabla v_{\theta}(s_t)$ . 
      Rewirte gradient : $\Delta \theta =  \sum_{k=0}^{T-1} \alpha \delta_{k} e_k$ .  Renaming by t : 
      $$
      \Delta \theta &=& \sum_{t=0}^{T-1} \alpha \delta_{t} \mathbf{e}_{t} \\
      e_t &=& \sum_{i=0}^{t}\gamma^{t-i} \nabla v_{\theta}(s_i) \\
      e_t &=& \gamma \sum_{i=0}^{t-1}\gamma^{t-i} \nabla v_{\theta}(s_i) + \nabla v_{\theta}(s_t) \\
      e_t &=& \gamma e_{t-1} + \nabla v_{\theta}(s_t)
      $$
      


 		5. In brief, we redistribute the computation at each step :
$$
\Delta \mathbf{w}_{t} \equiv \alpha \delta_{t} \mathbf{e}_{t} \\
\mathbf{e}_{t}=\gamma \mathbf{e}_{t-1}+\nabla v_{\theta}(s_t)
$$




### TD in Practice

1. What if we have finite horizon ?
   1. If we reach the goal: $V(s_t) = r_t$ , then the **Target should be $r_t$  and advantage should also be $r_t - V(s_t)$** . ( I have some doubts here, that is to say ; $G_t = r_t$) 
   2. if we run out of time: and $s_{t+1}$ is not the terminal state, then to calculate the target and advantage, we should use $V(s_{t+1})$.  The target should be $r_t + \gamma * V(s_{t+1})$ , and the advantage by GAE for the last step is simply the TD error


## Compare MC vs. TD

### Bootstrapping and Sampling

1. Bootstrapping: update involves an estimate

   - MC does not bootstrap

   - DP bootstraps

   - TD bootstraps

2. Sampling: update samples an expectation

   - MC samples

   - DP does not sample

   - TD samples



### Advantages & Disadvantages
1. TD-learning's <u>advantages</u>:
   1. **model-free** + learn from experience
   2. **incomplete episodes** + bootstraping + **online-learning**
   3. **Efficient: independent of temporal span** : TD can learn from 1 transition
   4. **Continuing Environments** : non-terminating
   5. **Low variance** : 
      1. TD' target depends on **1** random tuple : action, transition and reward
      2. MC' return depends on **many** random tuples : action, transition and reward
   6. **Off-Policy** or **Batch** : if we just have fixed batch of experience, for example K episodes with T transitions
      1. TD converges to **solution of max likelihood Markov model** 
      2. TD is helpful in **Fully observable environments**
2. MC's <u>advantages</u>:
   1. the estimate of value is **Unbiased**
   2. MC would accout for all **latent variables**.
   3. **Off-Policy** or **Batch** : if we just have fixed batch of experience, for example K episodes with T transitions
      1. MC converges to best **mean-squared fit for the observed returns**. 
      2. **MC does not exploit Markov Property**, may be useful in <u>Partially-observable environment</u> 



# DQN

0. Combine many techniques to propose a better **Q-Learning**

1. Recap of problems for Q- learning : 
   1. online iteration : sequential states are strongly correlated 
   2. one-step TD learning: target value is always changing 



## Replay Buffer

1. Key Idea : 
   1. not online updating
   2. **random sample** data from a buffer  to update
   3. update buffer 

2. Benefits : 
   1. we can **use any policy** to collect samples
   2. sample are **not correlated**
   3. multiple samples in the batch leads to **low-variance gradient** 



## Target Networks (Not same as Double Q-Learning)

0. **Pratically, worse than double q-learning** 

1. Use a taget Q function to generate target, and update the parameters every N steps. Inside 1 Step, use target Q function to compute multiple gradients

2. Classic Deep Q-learning 

   <img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-30 at 4.42.14 PM.png" alt="Screen Shot 2022-01-30 at 4.42.14 PM" style="zoom: 50%;" />

3. How to update target Q-function
   1. Update target every N steps : maximal lag N steps
   2. Polyak Averaging : update $\phi^{\prime}: \phi^{\prime} \leftarrow \tau \phi^{\prime}+(1-\tau) \phi \quad \tau=0.999$ works well 



## General View for DQN

<img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-30 at 4.46.22 PM.png" alt="Screen Shot 2022-01-30 at 4.46.22 PM" style="zoom:50%;" />



1. Three Process:
   1. Process1 : Data collection 
   2. Process2: Target Update 
   3. Process3: Q-function regression
2. Compare with original methods:
   1. Online Q-Learning : process 1,2 and 3 are in the same time + evict old data immediately 
   2. Offline Q-Learning : Process 3 in the inner loop of process 2 
## More Improvement for DQN

1. **Double Q-Learning** for overestimation : 

   1. $E\left[\max \left(X_{1}, X_{2}\right)\right] \geq \max \left(E\left[X_{1}\right], E\left[X_{2}\right]\right)$ 

   2. $\max _{\mathbf{a ^ { \prime }}} Q_{\phi^{\prime}}\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)=Q_{\phi^{\prime}}\left(\mathbf{s}^{\prime}, \arg \max _{\mathbf{a}^{\prime}} Q_{\phi^{\prime}}\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)\right)$ , action choice and action evaluation are highly correlated

   3. **Idea: don't use the same network to choose the action and evaluate value** 
      $$
      Q_{\phi_{A}}(\mathbf{s}, \mathbf{a}) \leftarrow r+\gamma Q_{\phi_{B}}\left(\mathbf{s}^{\prime}, \arg \max _{\mathbf{a}^{\prime}} Q_{\phi_{A}}\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)\right)\\
      Q_{\phi_{B}}(\mathbf{s}, \mathbf{a}) \leftarrow r+\gamma Q_{\phi_{A}}\left(\mathbf{s}^{\prime}, \arg \max _{\mathbf{a}^{\prime}} Q_{\phi_{B}}\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)\right)
      $$
      

2. **Multi-step Returns** 
   1. Change the target : $y_{j, t}=\sum_{t^{\prime}=t}^{t+N-1} \gamma^{t-t^{\prime}} r_{j, t^{\prime}}+\gamma^{N} \max _{\mathbf{a}_{j, t+N}} Q_{\phi^{\prime}}\left(\mathbf{s}_{j, t+N}, \mathbf{a}_{j, t+N}\right)$ 
      1. **Less biased target value** 
      2. **Faster Learning inherited from MC**
   2. **Problem : ** 
      1. need to be on-policy since the culmulative reward should be sampled from the same policy
      2. Otherwise, we need to use importance sampling
3. **Continuous Action** : 
   1. Souce of Problem : **Max Operator** for target value is hard to realize in high dimension and continuous action. 
   2. Option : 
      1. **Stochastic Optimization** : 
         1. $\max _{\mathbf{a}} Q(\mathbf{s}, \mathbf{a}) \approx \max \left\{Q\left(\mathbf{s}, \mathbf{a}_{1}\right), \ldots, Q\left(\mathbf{s}, \mathbf{a}_{N}\right)\right\}$
         2. CEM (cross-entropy method), CMA-ES (Covariance matrix adaptation evolution strategy) 
      2. **Easily maximizable Q-Function**: NAF
         1. normalized advantage functions : $Q_{\phi}(\mathbf{s}, \mathbf{a})=-\frac{1}{2}\left(\mathbf{a}-\mu_{\phi}(\mathbf{s})\right)^{T} P_{\phi}(\mathbf{s})\left(\mathbf{a}-\mu_{\phi}(\mathbf{s})\right)+V_{\phi}(\mathbf{s})$ 
            <img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-30 at 5.10.55 PM.png" alt="Screen Shot 2022-01-30 at 5.10.55 PM" style="zoom:33%;" />
         2. $\arg \max _{\mathbf{a}} Q_{\phi}(\mathbf{s}, \mathbf{a})=\mu_{\phi}(\mathbf{s}) \quad \max _{\mathbf{a}} Q_{\phi}(\mathbf{s}, \mathbf{a})=V_{\phi}(\mathbf{s})$ 
      3. **Learn an approximate maximizer** : DDPG 
         1. Learn a action value maximizer : network $\mu_{\theta}(\mathbf{s})$ such that $\mu_{\theta}(\mathbf{s}) \approx \arg \max _{\mathbf{a}} Q_{\phi}(\mathbf{s}, \mathbf{a})$ 
         2. new target : $y_{j}=r_{j}+\gamma Q_{\phi^{\prime}}\left(\mathbf{s}_{j}^{\prime}, \mu_{\theta}\left(\mathbf{s}_{j}^{\prime}\right)\right) \approx r_{j}+\gamma Q_{\phi^{\prime}}\left(\mathbf{s}_{j}^{\prime}, \arg \max _{\mathbf{a}^{\prime}} Q_{\phi^{\prime}}\left(\mathbf{s}_{j}^{\prime}, \mathbf{a}_{j}^{\prime}\right)\right)$ 
            <img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-30 at 5.15.43 PM.png" alt="Screen Shot 2022-01-30 at 5.15.43 PM" style="zoom:50%;" />



## More Tips

1. Q-learning takes some care to stabilize. 
2. Large replay buffers help improve stability. 
3. Take much time !
4. Start with high exploration and gradually reduce. 
5. Bellman error could be big -> use clip gradient or use Huber Loss
6. Always use Double Q-Learning
7. Try N-steps, schedule exploration, different learning rates and Adam.
8. More Reading : 
   1. Classic papers
      1. Watkins. (1989). Learning from delayed rewards: introduces Q-learning
      2. Riedmiller. (2005). Neural fitted Q-iteration: batch-mode Q-learning with neural networks
   2. Deep reinforcement learning Q-learning papers
      1. Lange, Riedmiller. (2010). Deep auto-encoder neural networks in reinforcement learning: early image-based Q-learning method using autoencoders to construct embeddings
      2. Mnih et al. (2013). Human-level control through deep reinforcement learning: Qlearning with convolutional networks for playing Atari.
      3. Van Hasselt, Guez, Silver. (2015). Deep reinforcement learning with double Q-learning: a very effective trick to improve performance of deep Q-learning.
      4. Lillicrap et al. (2016). Continuous control with deep reinforcement learning: continuous Q-learning with actor network for approximate maximization.
      5. Gu, Lillicrap, Stuskever, L. (2016). Continuous deep Q-learning with model-based acceleration: continuous Q-learning with action-quadratic value functions.
      6. Wang, Schaul, Hessel, van Hasselt, Lanctot, de Freitas (2016). Dueling network architectures for deep reinforcement learning: separates value and advantage estimation in Q-function.
