# Exploration

0. Landscape of Exploration:
   <img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-23 at 9.03.01 PM.png" alt="Screen Shot 2022-01-23 at 9.03.01 PM" style="zoom:50%;" />
0. **General Themes**:
   1. **UCB**; counting based
   2. **Thompson Sampling**; bayesian approches
   3. **Info Gain**
   

## Multi-Armed Bandit
### Classic Setting
1. Bandits: elements:
	1. **Action** :  $A_{t} \in\{1, \ldots, K\}$, each time we choose the action a, we will receive a reward  $R_{t} \in \mathbb{R}$
	2. **Expeced value for action a** : $q_{*}(a)=\mathbb{E}_{R_{t} \sim \nu_{a}}\left[R_{t} \mid A_{t}=a\right]$
	   1. $\nu\left(R_{t} \mid A_{t}=a\right) \equiv \nu_{a}\left(R_{t}\right)$, here each $\nu_{a}$ corresponds to a **distribution**
	   
	3. **Generic bandits framework**: for t =  1 to T:
	      		1. Take <u>action</u> $A_{t} \sim \pi$
	            		2. Observe <u>reward</u> $R_{t}$ (Note: this $R_{t}$ only wrt $A_{t}$ )
	            		3. Update the estimated <u>distribution</u> [if we want to estimate];
	            	  Update the agent <u>policy</u> $\pi$
	            		4. PS: why do need the **sequence of decision ?**
	            	  		1. policy is based on the estimated distribution
	            	  		2. estimated distribution need to be updated according to the interaction [action - reward]
	            	  		3. **Goal of Policy：** help estimate the distribution + maximize the reward 【not the cultimated reward】
	4. **Reward and Regret**
	   1. **mean reward** of an arm (at time t, time t is not important since arm is **stationary**)
	                           	   $\mu(a):=\mathbb{E}_{R_{t} \sim \nu_{a}}\left[R_{t} \mid A_{t}=a\right]$
	                           
	   2. **Best reward** : $\mu^{*}:=\max _{a \in \mathcal{A}} \mu(a) \quad a^{*}=\underset{a \in \mathcal{A}}{\operatorname{argmax}} \mu(a)$
	      
	   3. **Regret: ** for period T and policy $\pi$
	       1. $\mathcal{R}_{T}=\mu^{*} T-\mathbb{E}_{\pi}\left[\sum_{t=1}^{T} R_{t}\right]$
	       2. Best we could have - what we really have
	   4. **Goal: ** minimize regret
	   5. **Trade-off : Exploitation and Exploration**

### Setting for RL

0. Same Idea as before: **learn a policy: distribution on Action, to maximize cumulative reward**; but we don't 
1. **Values and Regret**:
   1. action value:   $q(a)=\mathbb{E}\left[R_{t} \mid A_{t}=a\right]$
   2. optimal value:  $v_{*}=\max _{a \in \mathcal{A}} q(a)=\max _{a} \mathbb{E}\left[R_{t} \mid A_{t}=a\right]$, max expectation value
   3. regret of an action: $\Delta_{a}=v_{*}-q(a)$,  <u>optimal action is zero regret</u>
      1. Total Regret: $L_{t}=\sum_{n=1}^{t} v_{*}-q\left(A_{n}\right)=\sum_{n=1}^{t} \Delta_{A_{n}}$

### Exploration: classic algorithm

0. **Action Value Estimator**:  a simple way to estimate the action value:
   1. $Q_{t}(a)=\frac{\sum_{n=1}^{t} \mathcal{I}\left(A_{n}=a\right) R_{n}}{\sum_{n=1}^{t} \mathcal{I}\left(A_{n}=a\right)}$
   2. **Online Update incrementally: ** $Q_{t}\left(A_{t}\right)=Q_{t-1}\left(A_{t}\right)+\alpha_{t} \underbrace{\left(R_{t}-Q_{t-1}\left(A_{t}\right)\right)}_{\text {error }}$, $\forall a \neq A_{t}: Q_{t}(a)=Q_{t-1}(a)$
      1. $\alpha_{t}=\frac{1}{N_{t}\left(A_{t}\right)} \quad$ and $\quad N_{t}\left(A_{t}\right)=N_{t-1}\left(A_{t}\right)+1$

#### Greedy

0. Select action based on **estimated action value** : always **highest value**  $A_{t}=\underset{a}{\operatorname{argmax}} Q_{t}(a)$
   1. according to theory: **greedy policy has linear expected total regret [greedy policy is deterministic]**

#### E-Greedy

0. $\pi_{t}(a)= \begin{cases}(1-\epsilon)+\epsilon /|\mathcal{A}| & \text { if } Q_{t}(a)=\max _{b} Q_{t}(b) \\ \epsilon /|\mathcal{A}| & \text { otherwise }\end{cases}$
1. **Not deterministic policy, but constant e leads to linear expected total regret**

#### UCB

0. Why UCB ? **Optimism in face of uncertainty, more uncertainty about this action value, more important to explore that action**

   1. First part: estimate the action value
   2. Second part: know the uncertainty

1. Math Foundation: Hoeffding's Inequality

   1. Let $X_{1}, \ldots, X_{n}$ be i.i.d. random variables in $[0,1]$ with true mean $\mu=\mathbb{E}[X]$, and let $\bar{X}_{t}=\frac{1}{n} \sum_{i=1}^{n} X_{i}$ be the sample mean. Then
      $$
      p\left(\bar{X}_{n}+u \leq \mu\right) \leq e^{-2 n u^{2}}
      $$
      
   2. For bandits:  $p\left(Q_{t}(a)+U_{t}(a) \leq q(a)\right) \leq e^{-2 N_{t}(a) U_{t}(a)^{2}}$
   
      1. $Q_t(a)$: estimation of action value
      2. $U_t(a)$: uncertainty of this action
      3. $q(a)$: unknown expected action value
      4. For a fixed probability **p** for all actions, we do not know real expected value q, but still we have a inaccurate estimation, so we try to be optimistic about all actions : 
         we observe this value:  $Q_{t}(a)+U_{t}(a)$, all the actions share the same proba smaller than the real one.
      5. That is to say: **Optimistic Estimator for expected action value**, may be overestimated but all share the same proba.
   
1. UCB policy based on the mentioned idea:

   1. $a_{t}=\underset{a \in \mathcal{A}}{\operatorname{argmax}} Q_{t}(a)+c \sqrt{\frac{\log t}{N_{t}(a)}}$
   2. Theoretical Guarantee: **logarithmic expected total regret** with c = $\sqrt{2}$：
      1. $L_{t} \leq 8 \sum_{a \mid \Delta_{a}>0} \frac{\log t}{\Delta_{a}}+O\left(\sum_{a} \Delta_{a}\right), \quad \forall t$
   


#### Policy Gradients

0. Do not Estimate the **action value**, but use the other parameters, here we pose **action preference** : $H_{t}(a)$
   1. Softmax Policy:  $\pi(a)=\frac{\mathrm{e}^{H_{t}(a)}}{\sum_{b} \mathrm{e}^{H_{t}(b)}}$
   2. Goal : **Update Policy Parameters to increase expected value** 
   3. Gradient Ascent:  $\theta_{t+1}=\theta_{t}+\alpha \nabla_{\theta} \mathbb{E}\left[R_{t} \mid \pi_{\theta_{t}}\right]$
1. **Gradient Bandits** : **Famous REINFORCE trick**
   1. $\begin{aligned} \nabla_{\theta} \mathbb{E}\left[R_{t} \mid \pi_{\theta}\right] &=\nabla_{\theta} \sum_{a} \pi_{\theta}(a) \overbrace{\mathbb{E}\left[R_{t} \mid A_{t}=a\right]}^{=q(a)} \\ &=\sum_{a} q(a) \nabla_{\theta} \pi_{\theta}(a) \\ &=\sum_{a} q(a) \frac{\pi_{\theta}(a)}{\pi_{\theta}(a)} \nabla_{\theta} \pi_{\theta}(a) \\ &=\sum_{a} \pi_{\theta}(a) q(a) \frac{\nabla_{\theta} \pi_{\theta}(a)}{\pi_{\theta}(a)} \\ &=\mathbb{E}\left[R_{t} \frac{\nabla_{\theta} \pi_{\theta}\left(A_{t}\right)}{\pi_{\theta}\left(A_{t}\right)}\right] \end{aligned}$
   2. First the randomness comes from **policy** and **reward** 
   3. $\nabla_{\theta} \mathbb{E}\left[R_{t} \mid \theta\right]=\mathbb{E}\left[R_{t} \nabla_{\theta} \log \pi_{\theta}\left(A_{t}\right)\right]$ , gradient of objective becomes an expectation, we could sampled reward and actions to estimate the gradient.
   4. For softmax: $\begin{aligned} H_{t+1}(a) &=H_{t}(a)+\alpha R_{t} \frac{\partial \log \pi_{t}\left(A_{t}\right)}{\partial H_{t}(a)} \\ &=H_{t}(a)+\alpha R_{t}\left(\mathcal{I}\left(a=A_{t}\right)-\pi_{t}(a)\right) \end{aligned}$


#### Bayesian Approches
0. Bayesian approches **model the reward distribution** of various arms. For example, each arm can be modelled as Bernoulli distribution. $\Theta \in [0,1]^{k}$, and  we adopt a model for the **prior distribution**. For example, beta distribution, a function of $\alpha$ and $\beta$
1. More formally : $P(\theta \mid D)=\frac{P(D \mid \theta) P(\theta)}{P(D)}$
   1. D is dataset
   2. $\theta$ supposed to be bernoulli with mean $\theta$, variance $\theta(1-\theta)$
   3. $P(D \mid \theta)=\theta^{k}(1-\theta)^{N-k}$
   4. $P(\theta)=\frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a, b)}$, where $B(a, b)=\frac{\Gamma(a) \Gamma(B)}{\Gamma(A+B)}$
   5. Each round after receiving the reward, we update the prior distribution
   6. Each round, we sample $\theta$ from beta distribution for each arm, and select the arm according to the sampled $\theta$
#### Thompson Sampling（Posterior sampling） 

<img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-23 at 9.45.16 PM.png" alt="Screen Shot 2022-01-23 at 9.45.16 PM" style="zoom: 50%;" />

1. Implementation with 

   1. arm model : bernoulli distribution
   2. prior distribution: beta distribution

2. Code Python:

   ```python
   class ThompsonSampling():
       def __init__(self, counts, values, a, b):
           self.counts = counts # Count represent counts of pulls for each arm. For multiple arms, this will be a list of counts.
           self.values = values # Value represent average reward for specific arm. For multiple arms, this will be a list of values.
           
           # Beta parameters
           self.a = a
           self.b = b
           return
       # Initialise k number of arms
       def initialize(self, n_arms):
           self.counts = [0 for col in range(n_arms)]
           self.values = [0.0 for col in range(n_arms)]
           # Uniform distribution of prior beta (A,B)
           self.a = [1 for arm in range(n_arms)]
           self.b = [1 for arm in range(n_arms)]
           return
       
       # Thompson Sampling selection of arm for each round
       def select_arm(self):
           n_arms = len(self.counts)
           
           # Pair up all beta params of a and b for each arm
           beta_params = zip(self.a, self.b)
           
           # Perform random draw for all arms based on their params (a,b)
           all_draws = [beta.rvs(i[0], i[1], size = 1) for i in beta_params]
           
           # return index of arm with the highest draw
           return all_draws.index(max(all_draws))
       
       # Choose to update chosen arm and reward
       def update(self, chosen_arm, reward):
           # update counts pulled for chosen arm
           self.counts[chosen_arm] = self.counts[chosen_arm] + 1
           n = self.counts[chosen_arm]
           
           # Update average/mean value/reward for chosen arm
           value = self.values[chosen_arm]
           new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward
           self.values[chosen_arm] = new_value
           
           # Update a and b
           
           # a is based on total counts of rewards of arm
           self.a[chosen_arm] = self.a[chosen_arm] + reward
           
           # b is based on total counts of failed rewards on arm
           self.b[chosen_arm] = self.b[chosen_arm] + (1-reward)
           
           return
   ```

#### Information Gain

0. Considering the bayesian experimental design : Exist a latent variable, observation could inform the latent variable.
   1. Latent variable $z$ : may be optimal action, value 
   2. Observation $y$ : received reward 
   3. $\mathcal{H}(\hat{p}(z))$ be the current entropy of our $z$ estimate
   4. $\mathcal{H}(\hat{p}(z) \mid y)$ be the entropy of our $z$ estimate after observation $y$
   5. **Information Gain: ** $\operatorname{IG}(z, y)=E_{y}[\mathcal{H}(\hat{p}(z))-\mathcal{H}(\hat{p}(z) \mid y)]$  
      1. Learn about z from actoin a

##### Example: Information Gain

1. $y=r_{a}, z=\theta_{a}\left(\right.$ parameters of model $\left.p\left(r_{a}\right)\right)$
2. $g(a)=\operatorname{IG}\left(\theta_{a}, r_{a} \mid a\right)-$ information gain of $a$
3. $\Delta(a)=E\left[r\left(a^{\star}\right)-r(a)\right]-$ expected suboptimality of $a$;  optimal reward - real world
4. policy : $\arg \min _{a} \frac{\Delta(a)^{2}}{g(a)}$
   1. numerator ： choose action minimize regret
   2. denominator ： choose action maximize the info gain
5. Source: [https://proceedings.neurips.cc/paper/2014/file/301ad0e3bd5cb1627a2044908a42fdc2-Paper.pdf]



## More than Multi-arm Bandits: Extended to MDPs

### UCB: Optimistic Exploration

0. Useful Source:
    1. [Unifying Count-Based Exploration and Intrinsic Motivation](https://proceedings.neurips.cc/paper/2016/file/afda332245e2af431fb7b672a68b659d-Paper.pdf)

1. **Exploration Bonus**:

   1. UCB in bandits:  $a=\arg \max \hat{\mu}_{a}+\sqrt{\frac{2 \ln T}{N(a)}}$ ->"exploration bonus"
   2. General Idea : $r^{+}(\mathbf{s}, \mathbf{a})=r(\mathbf{s}, \mathbf{a})+\mathcal{B}(N(\mathbf{s}))$

2. **Novelty-Seeking Exploration**

   1. counting by hashes, good hash function
   2. [classifier : whether the state is visited](https://arxiv.org/pdf/1703.01260.pdf)
   3. [couts via error](https://arxiv.org/pdf/1810.12894.pdf%20http://arxiv.org/abs/1810.12894.pdf) 

3. Limits:

   1. not tractable in large state space

   2. can not distinguish continuous state

      

4. Examples:
   1. Fit generative models for <u>state distribution</u> : $p_{\theta}(\mathbf{s})$
      1. Classic Bonus:
         1. UCB:  $\mathcal{B}(N(\mathbf{s}))=\sqrt{\frac{2 \ln n}{N(\mathbf{s})}}$
         2. MBIE-EB:  $\mathcal{B}(N(\mathbf{s}))=\sqrt{\frac{1}{N(\mathbf{s})}}$
         3. BEB: $\mathcal{B}(N(\mathbf{s}))=\frac{1}{N(\mathbf{s})}$

### Posterior Sampling : From Thompson to RL

1. [Bootstrap DQN](https://arxiv.org/pdf/1602.04621.pdf)
   1. Thompson samples $\theta$ from prior distribution, and we assume that $\theta$ is directly related to the policy
   2. Same Idea: sample a q function from q function buffer
2. Why Bootstrap DQN works ?
   1. **Q-learning** is off-policy, so **data could be shared within different Q function**
   2. epsilon-greedy may oscillate back and forth; **random Q-functions has internally consistent strategy for an entire episode.**



### Infomation Gain

1. General Theme:  Info gain: $\quad \operatorname{IG}(z, y \mid a)$
2. Information about waht ?
   1. reward : problem when reward is sparse
   2. state density 
   3. dynamics : good proxy for learning MDP
3. Examples:
   1. [VIME: Variational Information Maximizing Exploration](https://arxiv.org/pdf/1605.09674.pdf?source=post_page---------------------------)
   2. [INCENTIVIZING EXPLORATION IN REINFORCEMENT LEARNING WITH DEEP PREDICTIVE MODELS](https://arxiv.org/pdf/1507.00814.pdf)
   3. [Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990–2010)](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5508364)



## Exploration without Reward Signals

### Information Theory

<img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-24 at 8.54.06 AM.png" alt="Screen Shot 2022-01-24 at 8.54.06 AM" style="zoom: 50%;" />

1. Entropoy: $\mathcal{H}(p(\mathbf{x}))=-E_{\mathbf{x} \sim p(\mathbf{x})}[\log p(\mathbf{x})]$
2. Mutual Information: $\begin{aligned} \mathcal{I}(\mathbf{x} ; \mathbf{y}) &=D_{\mathrm{KL}}(p(\mathbf{x}, \mathbf{y}) \| p(\mathbf{x}) p(\mathbf{y})) \\ &=E_{(\mathbf{x}, \mathbf{y}) \sim p(\mathbf{x}, \mathbf{y})}\left[\log \frac{p(\mathbf{x}, \mathbf{y})}{p(\mathbf{x}) p(\mathbf{y})}\right] \\ &=\mathcal{H}(p(\mathbf{y}))-\mathcal{H}(p(\mathbf{y} \mid \mathbf{x})) \end{aligned}$

### Topcis
#### Goal-Based

0. Prepare for an unknown future goal:
   1. [Visual Reinforcement Learning with Imagined Goals](https://arxiv.org/pdf/1807.04742.pdf)
   2. [Skew-Fit: State-Covering Self-Supervised Reinforcement Learning](https://arxiv.org/pdf/1903.03698.pdf)

#### Distribution-Matching

0. [Efficient Exploration via State Marginal Matching](https://arxiv.org/pdf/1906.05274.pdf)
1. [Provably Efficient Maximum Entropy Exploration](https://arxiv.org/pdf/1812.02690.pdf)

#### Covering space of skills

2. [Unsupervised Meta-Learning for Reinforcement Learning](https://arxiv.org/pdf/1806.04640.pdf)
3. [DIVERSITY IS ALL YOU NEED](https://arxiv.org/pdf/1802.06070.pdf)
4. [VARIATIONAL INTRINSIC CONTROL](https://arxiv.org/pdf/1611.07507.pdf)

