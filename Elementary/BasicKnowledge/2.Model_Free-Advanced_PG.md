# Advanced Policy Gradient
0. Preface : recap in policy gradient, we used **approximation** in importance sampling if we use the buffer to train the policy. 
1. **Why the policy gradient in algorithm like REINFORCE ? Why the gradient could not let us have better results ?** 
   1. if objective function $J_{\theta}(X)$ is evaluated on the fixed dataset $X$ , then of course the gradient ascents could make the $J_{\theta^{\prime}}(X) > J_{\theta}(X)$ 
   2. But the Reinforcement Learning has a different framework ! Why ? the **$X$ is collected by policy determined by $\theta$**  . That is to say, $J_{\theta^{\prime}}(X_{\theta}) > J_{\theta}(X_{\theta})$  . but not $J_{\theta^{\prime}}(X_{\theta^{\prime}}) > J_{\theta}(X_{\theta})$  !!! Since the data set becomes different !!! 


## Distribution Mismatch
### Rewrite Policy Gradient
1. Objective Function (with infinite horizon) : 
   - $J(\theta)=E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t} \gamma^{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]$
   - Source of randomness : the trajectory. If the parameter of policy changed, you will probably not choose the action at that state.
2. Improvement if we update the parameters of policy:
   - ​	$J\left(\theta^{\prime}\right)-J(\theta)$  = $E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t} \gamma^{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right] - E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t} \gamma^{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]$ . Ops, expectation with respect to different randomness. We need to rewrite the improvement.
   - $\begin{aligned} J\left(\theta^{\prime}\right)-J(\theta) &=J\left(\theta^{\prime}\right)-E_{\mathbf{s}_{0} \sim p\left(\mathbf{s}_{0}\right)}\left[V^{\pi_{\theta}}\left(\mathbf{s}_{0}\right)\right] \\ &=J\left(\theta^{\prime}\right)-E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[V^{\pi_{\theta}}\left(\mathbf{s}_{0}\right)\right] \end{aligned}$ 
     - Here, we assume that agent always starts at the same state.
   - (trick of infinite horizon) : $=J\left(\theta^{\prime}\right)-E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t} V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)-\sum_{t=1}^{\infty} \gamma^{t} V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)\right]$ 
   - (reverse the order of t=1 and t=0, and we could change the sign before the second term ) : $=J\left(\theta^{\prime}\right)+E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(\gamma V^{\pi_{\theta}}\left(\mathbf{s}_{t+1}\right)-V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)\right)\right]$  
   - (Expand the first term and rewrite in terms of **advantage**) :$=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\gamma V^{\pi_{\theta}}\left(\mathbf{s}_{t+1}\right)-V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)\right)\right]$
   - $=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]$ 
3. **Claim: ** $J\left(\theta^{\prime}\right)-J(\theta)=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]$ 
   1. **Trajectory collected by "new" policy**
   2. **Advantage evaluated by old parameter** 

### Rethink of importance sampling

0. **Why do we care about the problem ?**
   1. First, we want to find the $\theta^{\prime}$ to maximize the expectation of advantages.
   2. We don't have access to true expectation. So we estimate by sampling.
   3. Second, we do have $\frac{1}{N}\sum_{a \sim \pi_{\theta}(a|s)}A^{\pi_{\theta}}(s,a)$. But it's the estimate of $E_{a_t \sim \pi_{\theta}}$ . Not what we want, so we fix what we want and modify the samples that we should collect.

1. **Expand the true expectation of advantage** :  $E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]=\sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\left[\gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]$ 
2. **First try: <u>Importance Sampling</u>** :
   1. $=\sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]$ 
   2. Here, we get the mismatch of distribution in the first expectation. 
   3. <u>if there we could use the old distribution ?</u>
3. **Second thinking: why do we want to use the old distribution ?** 
   1. if so, the advantages become  $=\sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]$   = $\bar{A}(\theta^{\prime})$
   2. **our goal becomes: ** $\theta^{\prime} \leftarrow \arg \max _{\theta^{\prime}} \bar{A}(\theta)$  
4. **Claim : $p_{\theta}\left(\mathbf{s}_{t}\right)$ is close to $p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)$ when $\pi_{\theta}$ is close to $\pi_{\theta^{\prime}}$**  [To prove. of course]



### Bounding the distribution

1. Some theoretical background

   1. if  $\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \neq \pi_{\theta}\left(\mathbf{s}_{t}\right) \mid \mathbf{s}_{t}\right) \leq \epsilon$ , then $\left.p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)=(1-\epsilon)^{t} p_{\theta}\left(\mathbf{s}_{t}\right)+\left(1-(1-\epsilon)^{t}\right)\right) p_{\text {mistake }}\left(\mathbf{s}_{t}\right)$ 

   2. $\left|p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)-p_{\theta}\left(\mathbf{s}_{t}\right)\right|=\left(1-(1-\epsilon)^{t}\right)\left|p_{\text {mistake }}\left(\mathbf{s}_{t}\right)-p_{\theta}\left(\mathbf{s}_{t}\right)\right| \leq 2\left(1-(1-\epsilon)^{t}\right)$

   3. (Useful identity) : $(1-\epsilon)^{t} \geq 1-\epsilon t$ for $\epsilon \in[0,1]$ 

   4. $$
      \begin{aligned}
      \left|p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)-p_{\theta}\left(\mathbf{s}_{t}\right)\right|=\left(1-(1-\epsilon)^{t}\right)\left|p_{\text {mistake }}\left(\mathbf{s}_{t}\right)-p_{\theta}\left(\mathbf{s}_{t}\right)\right| & \leq 2\left(1-(1-\epsilon)^{t}\right) \\
      & \leq 2 \epsilon t
      \end{aligned}
      $$





2. **What makes TRPO a good paper ? the approximation depends on the horizon and other constants. It seems to be a huge error ?**

   1. $\begin{aligned} E_{p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)}\left[f\left(\mathbf{s}_{t}\right)\right]=\sum_{\mathbf{s}_{t}} p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right) f\left(\mathbf{s}_{t}\right) & \geq \sum_{\mathbf{s}_{t}} p_{\theta}\left(\mathbf{s}_{t}\right) f\left(\mathbf{s}_{t}\right)-\left|p_{\theta}\left(\mathbf{s}_{t}\right)-p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)\right| \max _{\mathbf{s}_{t}} f\left(\mathbf{s}_{t}\right) \\ & \geq E_{p_{\theta}}\left(\mathbf{s}_{t}\right)\left[f\left(\mathbf{s}_{t}\right)\right]-2 \epsilon t \max _{\mathbf{s}_{t}} f\left(\mathbf{s}_{t}\right) \end{aligned}$ 

   2. **In Advantage's setting, the true objective function is actually the upper bound of what we have (in estimation), so maximizing the estimated advantages do improve the true obejctive function.** 

   3. $$
      \begin{aligned}
      &\sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right] \geq \\
      &\sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]-\sum_{t} 2 \epsilon t C
      \end{aligned} 
      $$



### Rewrite everything ! 

1. New (Tractable) objective function with (Simple) constraints: 

2. $$
   \theta^{\prime} \leftarrow \arg \max _{\theta^{\prime}} \sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]
   $$
   such that $\left|\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)-\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right| \leq \epsilon$
   for small enough $\epsilon$, this is guaranteed to improve $J\left(\theta^{\prime}\right)-J(\theta)$ 

​	

3. **What's the next step ?**
   1. Maybe could find the theta maximizing the objective by old-school way like Hasting-metropolis ? But we have to obey the constraints since large step violates the objective function. 
   2. Probably **gradient ascend** starting from the $\theta$ [natural idea]. 

## Policy Gradients with Constraints

1. Holding the buffer collected by old policy ? No, you can not use the policy gradient directly !!! **At least importance sampling** (that means you need collect the old policy's probability to revise the estimation). **Importance Sampling is not enough**, it does not correct the mismatch of trajectory distribution. So here we are ~ **policy gradient with constraints**.  

### Rewrite the constraints

1. Tractable constraints with good property : 

   1. $\left|\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)-\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right| \leq \sqrt{\frac{1}{2} D_{\mathrm{KL}}\left(\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right) \| \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right)}$ 
   2. So **KL divergence is the upper bound of original constraints** 
   2. **We should use estimator for KL divergence since it's still an expectation** :  $D_{\mathrm{KL}}\left(p_{1}(x) \| p_{2}(x)\right)=E_{x \sim p_{1}(x)}\left[\log \frac{p_{1}(x)}{p_{2}(x)}\right]$ 

2. **New Optimization Problem** :  

3. $$
   \theta^{\prime} \leftarrow \arg \max _{\theta^{\prime}} \sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]
   $$
   such that $D_{\mathrm{KL}}\left(\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right) \| \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right) \leq \epsilon$ 



### Dual Gradient Descent
1. Rewrite the objective function in form of **Lagragian**
1. $\mathcal{L}\left(\theta^{\prime}, \lambda\right)=\sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]-\lambda\left(D_{\mathrm{KL}}\left(\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right) \| \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right)-\epsilon\right)$
3. **Now it becomes classic (tractable ?) optimization problem** 
   1. Maximize $\mathcal{L}\left(\theta^{\prime}, \lambda\right)$ with respect to $\theta^{\prime}$
   2. $\lambda \leftarrow \lambda+\alpha\left(D_{\mathrm{KL}}\left(\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right) \| \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right)-\epsilon\right)$ 
### Natural Policy Gradient

0. **Natural Gradient takes root from <u>First-order approximation</u>** 
1. $\theta^{\prime} \leftarrow argmax_{\theta^{\prime}} \bar{A}\left(\theta^{\prime}\right) $   
2. $\theta^{\prime} \leftarrow argmax_{\theta^{\prime}} \bar{A}(\theta) + \nabla_{\theta}\bar{A}(\theta)(\theta^{\prime} - \theta) $  =  $\theta^{\prime} \leftarrow \arg \max _{\theta^{\prime}} \nabla_{\theta} \bar{A}(\theta)^{T}\left(\theta^{\prime}-\theta\right)$ 

### Trust Region Policy Gradient