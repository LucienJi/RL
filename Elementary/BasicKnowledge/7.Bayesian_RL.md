# Bayesian Reinforcement Learning
0. Preface
   1. Major Challenge in RL : **Identify good data collection strategies -> balance between the need to explore and exploit (since policy prefer the goal-oriented path)** :
   2. Bayesian Inference : designer of the system could express the prior information about the problem in a probabilistic distribution. Informaiton could be encoded and updated by some parametric representation. 


## Maths Framework
### Contextual Bandit

1. Model  (**Contextual Bandit**) Define a contextual bandit to be a tuple $\left\langle\mathcal{S}, \mathcal{A}, \mathcal{Y}, P_{S}, P, r\right\rangle$ where

   - $\mathcal{S}$ is the set of contexts,

   - $\mathcal{A}$ is the set of actions (arms),

   - $\mathcal{Y}$ is the set of possible outcomes,

   - $P_{S}(\cdot) \in \mathcal{P}(\mathcal{S})$ is the context probability,

   - $P(\cdot \mid a, s) \in \mathcal{P}(\mathcal{Y})$ is the outcome probability, conditioned on action $a \in \mathcal{A}$ being taken when the context is $s \in \mathcal{S}$,

   - $r(Y) \in \mathbb{R}$ represents the reward obtained when outcome $Y \in \mathcal{Y}$ is observed.

2. Difference with Multi-arm Bandit ?

   1. At each time step t, the decision maker first **observes a context** s ∈ S, drawn i.i.d. over time from a probability distribution Ps(·) ∈ P(S), 
   2. The decision-maker then chooses an action a ∈ A and observes a random outcome $Y_{t}\left(a_{t}, s_{t}\right) \in \mathcal{Y}$ . 

### POMDP

1. Model(**Partially Observable Markov Decision Process**) Define a POMDP $\mathcal{M}$ to be a tuple $\left\langle\mathcal{S}, \mathcal{A}, \mathcal{O}, P, \Omega, P_{0}, q\right\rangle$ where

   - $\mathcal{S}$ is the set of states,

   - $\mathcal{A}$ is the set of actions,

   - $\mathcal{O}$ is the set of observations,

   - $P(\cdot \mid s, a) \in \mathcal{P}(\mathcal{S})$ is the probability distribution over next states, conditioned on action $a$ being taken in state $s$,

   - $\Omega(\cdot \mid s, a) \in \mathcal{P}(\mathcal{O})$ is the probability distribution over possible observations, conditioned on action $a$ being taken to reach state $s$ where the observation is perceived,

   - $P_{0} \in \mathcal{P}(\mathcal{S})$ is the probability distribution according to which the initial state is selected,

   - $R(s, a) \sim q(\cdot \mid s, a) \in \mathcal{P}(\mathbb{R})$ is a random variable representing the reward obtained when action $a$ is taken in state $s$.

2. **Belief** : since state is not observed, the agent **rely on the recent history of action and observation** to infer a distribution over current state : 

   1. $b_{t+1}\left(s^{\prime}\right)=\frac{\Omega\left(o_{t+1} \mid s^{\prime}, a_{t}\right) \int_{\mathcal{S}} P\left(s^{\prime} \mid s, a_{t}\right) b_{t}(s) d s}{\int_{\mathcal{S}} \Omega\left(o_{t+1} \mid s^{\prime \prime}, a_{t}\right) \int_{\mathcal{S}} P\left(s^{\prime \prime} \mid s, a_{t}\right) b_{t}(s) d s d s^{\prime \prime}}$ 
   2. **Explain: **  belief is <u>the probability of state conditioned on the current observation and action</u> . 
   3. $b_{t+1}(s^{\prime}) = P(s_{t+1}\mid o_{t+1})$ 

# Appendix
1. Souerce : Bayesian Reinforcement Learning: A Survey 
2. 