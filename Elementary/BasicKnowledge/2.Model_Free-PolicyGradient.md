# Policy Gradient

## (Dis)Advantages of Policy-Based RL

1. **True Objective**: each update is guaranteed to improve next episodic cumulative rewards
2. **Easy extended to High-dimensinal and Continuous Action Space**
3. **Stochastic** Policy, good for exploration
4. Limits:
   1. Local Optima
   2. Policy tends to be task-specific


## Goals and Setting

1. **Trajectory: ** based on the policy ($\theta$)
   1. $\underbrace{p_{\theta}\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \ldots, \mathbf{s}_{T}, \mathbf{a}_{T}\right)}_{p_{\theta}(\tau)}=p\left(\mathbf{s}_{1}\right) \prod_{t=1}^{T} \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right) p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_{t}, \mathbf{a}_{t}\right)$
2. **Goals**:  maximize the ( discounted ) cumulative reward
   1. General Theme: $\theta^{\star}=\arg \max _{\theta} E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]$
   2. Infinite Horizon:  $\theta^{\star}=\arg \max _{\theta} E_{(\mathbf{s}, \mathbf{a}) \sim p_{\theta}(\mathbf{s}, \mathbf{a})}[r(\mathbf{s}, \mathbf{a})]$
   3. Finite Horizon:  $\theta^{\star}=\arg \max _{\theta} \sum_{t=1}^{T} E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim p_{\theta}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]$
3. **Objective Function**: $\theta^{\star}=\arg \max _{\theta} \underbrace{E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]}_{J(\theta)}$
   1. Expectation:  $J(\theta)=E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]$
   2. Sampled:  $\frac{1}{N} \sum_{i} \sum_{t} r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)$

## Evaluating the Policy Gradient

4. **Differentiation**:
   1. sampled objective can not be differetiated
   2. **Calculate the expected differentiation**:
      1. Objective: $J(\theta)=E_{\tau \sim p_{\theta}(\tau)}[\underbrace{r(\tau)}_{\underline{\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}}]=\int p_{\theta}(\tau) r(\tau) d \tau$
      2. Diff:  $\nabla_{\theta} J(\theta)=\int \nabla_{\theta} p_{\theta}(\tau) r(\tau) d \tau=\int p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau) r(\tau) d \tau=E_{\tau \sim p_{\theta}(\tau)}\left[\nabla_{\theta} \log p_{\theta}(\tau) r(\tau)\right]$
      3. Now we sample to calculate the differentiation
   3. **Differentiation in details:**
      1. Diff:  $\nabla_{\theta} J(\theta)=E_{\tau \sim p_{\theta}(\tau)}\left[\nabla_{\theta} \log p_{\theta}(\tau) r(\tau)\right]$
      2. Where does our **Policy** intervene ?
         1. **Probability of trajectory** : 
            1. $\underbrace{p_{\theta}\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \ldots, \mathbf{s}_{T}, \mathbf{a}_{T}\right)}_{p_{\theta}(\tau)}=p\left(\mathbf{s}_{1}\right) \prod_{t=1}^{T} \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right) p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_{t}, \mathbf{a}_{t}\right)$
            2. $\log p_{\theta}(\tau)=\log p\left(\mathbf{s}_{1}\right)+\sum_{t=1}^{T} \log \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)+\log p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_{t}, \mathbf{a}_{t}\right)$
         2. **Simplification for differentiation** : 
            1. $\nabla_{\theta} \log p_{\theta}(\tau)$ = $\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)$
            2. **Final Formula**: $\nabla_{\theta} J(\theta)=E_{\tau \sim p_{\theta}(\tau)}\left[\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)\right]$
      3. **Sample the Diff**:
         1. ==Evaluating the gradient== : $\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N}\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)\right)$

## Understanding the Policy Gradient

1. Comparison to maximum likelihood :
   1. Policy Gradient:  $\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N}\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)\right)$
   2. Maximum Likelihood:  
      1. Probability of trajectory:  $\underbrace{p_{\theta}\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \ldots, \mathbf{s}_{T}, \mathbf{a}_{T}\right)}_{p_{\theta}(\tau)}=p\left(\mathbf{s}_{1}\right) \prod_{t=1}^{T} \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right) p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_{t}, \mathbf{a}_{t}\right)$
      2. Log Probabilityï¼š $\log p_{\theta}(\tau)=\log p\left(\mathbf{s}_{1}\right)+\sum_{t=1}^{T} \log \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)+\log p\left(\mathbf{s}_{t+1} \mid \mathbf{s}_{t}, \mathbf{a}_{t}\right)$
      3. **Maximizet the log probability of trajectory: **  $\nabla_{\theta} J_{\mathrm{ML}}(\theta) \approx \frac{1}{N} \sum_{i=1}^{N}\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}\right)\right)$





## Algorithms
### REINFORCE

1. REINFORCE
   1. sample $\left\{\tau^{i}\right\}$ from $\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)$ (run the policy)
   2. $\nabla_{\theta} J(\theta) \approx \sum_{i}\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{t}^{i} \mid \mathbf{s}_{t}^{i}\right)\left(\sum_{t^{\prime}=t}^{T} r\left(\mathbf{s}_{t^{\prime}}^{i}, \mathbf{a}_{t^{\prime}}^{i}\right)\right)\right)$
   3. $\theta \leftarrow \theta+\alpha \nabla_{\theta} J(\theta)$ 




# Reducing Variance

## Reward to Go
1. **Causality: ** policy at time $t^{\prime}$ cannot affect reward at time $t$ when $t<t^{\prime}$
   1. $\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} (\nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}\right) \underbrace{\left(\sum_{t^{\prime} = t}^{T} r\left(\mathbf{s}_{i, t^{\prime}}, \mathbf{a}_{i, t^{\prime}}\right)\right)}_{\text {"reward to go" }})$


## Baselines

1. **Subtract a baseline**: which condition should we satisfy ?
   1. **Theoretical Requirement: **  $E\left[\nabla_{\theta} \log p_{\theta}(\tau) b\right]=\int p_{\theta}(\tau) \nabla_{\theta} \log p_{\theta}(\tau) b d \tau=\int \nabla_{\theta} p_{\theta}(\tau) b d \tau=b \nabla_{\theta} \int p_{\theta}(\tau) d \tau=b \nabla_{\theta} 1=0$
   2. $\int \nabla_{\theta} p_{\theta}(\tau) b d \tau$  = $\nabla_{\theta} \textbf{E}_{\tau}(b)$, b should not depend on the $\theta$
2. For example:
   1. $b=\frac{1}{N} \sum_{i=1}^{N} r(\tau)$

### Actor-Critic Evaluation 

<img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-24 at 3.09.02 PM.png" alt="Screen Shot 2022-01-24 at 3.09.02 PM" style="zoom: 50%;" />



0. Use estimated value to act as the baseline
1. **Reward to Go**: $\sum_{t^{\prime}=1}^{T} r\left(\mathbf{s}_{i, t^{\prime}}, \mathbf{a}_{i, t^{\prime}}\right)$ , it's a estimate of expected value for reward-to-go
   1. **Expected reward-to-go**:  $Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=\sum_{t^{\prime}=t}^{T} E_{\pi_{\theta}}\left[r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right) \mid \mathbf{s}_{t}, \mathbf{a}_{t}\right]$
   2. **Reformulate Reward-to-go**  $\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}\right) Q\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)$
2. **Baseline: average reward-to-go**: we have posed the average value as the baseline : 
   1. **Extract baseline from Reward-to-go: ** $\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}\right)\left(Q\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)-V\left(\mathbf{s}_{i, t}\right)\right)$
      1. $V\left(\mathbf{s}_{t}\right)=E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\left[Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]$ 
      2. **Advantages: ** $A^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=Q^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-V^{\pi}\left(\mathbf{s}_{t}\right):$ how much better $\mathbf{a}_{t}$ is
3. **Value Function Fitting**: for $V^{\pi}$
   1. **Monte Carlo Target** : $V^{\pi}\left(\mathbf{s}_{t}\right) \approx \sum_{t^{\prime}=t}^{T} r\left(\mathbf{s}_{i, t^{\prime}}, \mathbf{a}_{i, t^{\prime}}\right) =  y_i$ 
   2. **Bootstrapped Estimate** :  $V^{\pi}\left(\mathbf{s}_{i,t}\right) $ $\approx r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)+\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i, t+1}\right) = y_i$ 
   3. supervised learning:  $\mathcal{L}(\phi)=\frac{1}{2} \sum_{i}\left\|\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i}\right)-y_{i}\right\|^{2}$ 

### Actor-Critic Algorithm
1. Batch vs. Online 
   <img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-24 at 3.22.36 PM.png" alt="Screen Shot 2022-01-24 at 3.22.36 PM" style="zoom: 50%;" /> <img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-24 at 3.23.04 PM.png" alt="Screen Shot 2022-01-24 at 3.23.04 PM" style="zoom:50%;" />

2. **Problem of online RL**: interacting while updating, but value function's update prefers working with a batch ( like using parallel workers, asynchronous parallel actor-critic )
   1. what's the problem if we use both **batch** and **online updating** : 
      1. update $\hat{V}_{\phi}^{\pi}$ using target $r+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}^{\prime}\right)$ : it's wrong, since $s^{\prime}$ will not the next state, since the policy has been changed
      2. $\nabla_{\theta} J(\theta) \approx \nabla_{\theta} \log \pi_{\theta}(\mathbf{a} \mid \mathbf{s}) \hat{A}^{\pi}(\mathbf{s}, \mathbf{a})$ : it's wrong, since the advantages are wrong and policy also suffers from off-policy problem
3. **Methods to correct RL** : 
   1. **Replace V with Q** : update $\hat{Q}_{\phi}^{\pi}$ using targets $y_{i}$ = $=r_{i}+\gamma \hat{Q}_{\phi}^{\pi}\left(\mathbf{s}_{i}^{\prime}, \mathbf{a}_{i}^{\prime}\right)$ , and the $a_{i}^{\prime}$ is not from buffer, it's new sampled by **new policy**
   2. **Use the same trick, just use original state without using old action**  : $\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i}^{\pi} \mid \mathbf{s}_{i}\right) \hat{A}^{\pi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}^{\pi}\right)$ , with sample $\mathbf{a}_{i}^{\pi} \sim \pi_{\theta}\left(\mathbf{a} \mid \mathbf{s}_{i}\right)$ 


## More on Baseline
### Eligibility traces & n-step returns
1. 2-way to calculate advantages: 

   1. Bootstraps: $\hat{A}_{\mathrm{C}}^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{t+1}\right)-\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{t}\right)$ : low varaince but biased

   2. Monte-Carlo:  $\hat{A}_{\mathrm{MC}}^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=\sum_{t^{\prime}=t}^{\infty} \gamma^{t^{\prime}-t} r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right)-\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{t}\right)$ : no-biased but high variance

      ![Screen Shot 2022-01-24 at 4.11.41 PM](/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-24 at 4.11.41 PM.png)

2. **N-step**

   1. **Rewrite the Reward to Go as n-step real reward and discounted value**: $\hat{A}_{n}^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=\sum_{t^{\prime}=t}^{t+n} \gamma^{t^{\prime}-t} r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right)-\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{t}\right)+\gamma^{n} \hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{t+n}\right)$ 
### Generalized Advantage EstimatioE (GAE)

1. No need to choose the **n-step** , use the weighted advantage instead: 
   1. **N-step** : $\hat{A}_{n}^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=\sum_{t^{\prime}=t}^{t+n} \gamma^{t^{\prime}-t} r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right)-\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{t}\right)+\gamma^{n} \hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{t+n}\right)$
   2. **GAE** : $\hat{A}_{\mathrm{GAE}}^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=\sum_{n=1}^{\infty} w_{n} \hat{A}_{n}^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$ , $w_{n} \propto \lambda^{n-1} \quad$ exponential falloff
   3. **Rewrite GAE: ** $\hat{A}_{\mathrm{GAE}}^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=\sum_{t^{\prime}=t}^{\infty}(\gamma \lambda)^{t^{\prime}-t} \delta_{t^{\prime}} \quad \delta_{t^{\prime}}=r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right)+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{t^{\prime}+1}\right)-\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{t^{\prime}}\right)$ 

##  Analyze Variance

1. **Write down the variance explictly** 
   1. $\operatorname{Var}[x]=E\left[x^{2}\right]-E[x]^{2}$
   2. $\nabla_{\theta} J(\theta)=E_{\tau \sim p_{\theta}(\tau)}\left[\nabla_{\theta} \log p_{\theta}(\tau)(r(\tau)-b)\right]$
   3. $\operatorname{Var}=E_{\tau \sim p_{\theta}(\tau)}\left[\left(\nabla_{\theta} \log p_{\theta}(\tau)(r(\tau)-b)\right)^{2}\right]-E_{\tau \sim p_{\theta}(\tau)}\left[\nabla_{\theta} \log p_{\theta}(\tau)(r(\tau)-b)\right]^{2}$
2. Simplification:
   1. Pose gradient as  $g(\tau)$
   2. calculate the differentiation on b, so we don't need to expand the second term
      1. $\frac{d \text { Var }}{d b}=\frac{d}{d b} E\left[g(\tau)^{2}(r(\tau)-b)^{2}\right]$
      2. Just keep the term with b: $\frac{dVar}{db} = -2 E\left[g(\tau)^{2} r(\tau)\right]+2 b E\left[g(\tau)^{2}\right]$
      3. Necessary Condition: diff = 0:
         1. $b=\frac{E\left[g(\tau)^{2} r(\tau)\right]}{E\left[g(\tau)^{2}\right]}$



# Off-Policy Policy Gradient

## Source of Problem

1. what we have talked about is on-policy update : $\nabla_{\theta} J(\theta)=E_{\tau \sim p_{\theta}(\tau)}\left[\nabla_{\theta} \log p_{\theta}(\tau) r(\tau)\right]$
   1. **Problem: randomness is originated from trajectory generated by policy**
   2. **On-policy is sometimes inefficient, we want to use other buffer**
      1. If we want to reuse the buffer for several updates, the parameters has been different.



## Importance Sampling

1. Math Foundation:

   1. importance sampling

   $$
   \begin{aligned}
   E_{x \sim p(x)}[f(x)] &=\int p(x) f(x) d x \\
   &=\int \frac{q(x)}{q(x)} p(x) f(x) d x \\
   &=\int q(x) \frac{p(x)}{q(x)} f(x) d x \\
   &=E_{x \sim q(x)}\left[\frac{p(x)}{q(x)} f(x)\right]
   \end{aligned}
   $$
   2. Reformulation:
      1. **Goal**: we want to estimate a value $E_{p}(f(X))$ where  $X \sim p $
      2. **Usually: ** unbiased estimation $\frac{1}{N}\sum_{t}^{N}f(x_t)$ if $x_i$ sampled from $p$
      3. **Off-Policy: ** if $x_i$ is sampled from $q$ , then the above estimator is <u>baised</u> :  $\frac{1}{N}\sum_{t}^{N}f(x_t)$  is an estimator for  $E_{q}(f(X))$ , by using importance sampling, we can transform it into $E_{p}(\frac{q(X)}{p(X)}f(X))$
         1. **Modified to Unbiased**: $\frac{1}{N}\sum_{t}^{N} \frac{p(x_t)}{q(x_t)}f(x_t)$ $\sim$  $E_{p}(f(X))$ , where $x \sim q$ 

2. **Source of Problem**: 
   1. Trajectory collected by old policy; that is to say : seeing state, now we will choose the different action, so the probability of $\pi(a\mid s)$ is different.
   2. **We now want to update** $\theta^{\prime}$, **but data is collected from** $\theta$ :  $\nabla_{\theta^{\prime}} J\left(\theta^{\prime}\right) = E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}(\tau) r(\tau)\right] \quad=E _{\tau \sim p_{\theta}(\tau)}\left[\frac{p_{\theta^{\prime}}(\tau)}{p_{\theta}(\tau)} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}(\tau) r(\tau)\right] \quad$ when $\theta \neq \theta^{\prime}$
      1.  = $E_{\tau \sim p_{\theta}(\tau)}\left[\left(\prod_{t=1}^{T} \frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)}\right)\left(\sum_{t=1}^{T} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)\right]$
      2.  **Too Complicated to calculate: Approximation**
         1. $\nabla_{\theta^{\prime}} J\left(\theta^{\prime}\right)=E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=1}^{T} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\left(\prod_{t^{\prime}=1}^{t} \frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t^{\prime}} \mid \mathbf{s}_{t^{\prime}}\right)}{\pi_{\theta}\left(\mathbf{a}_{t^{\prime}} \mid \mathbf{s}_{t^{\prime}}\right)}\right)\left(\sum_{t^{\prime}=t}^{T} r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right)\right)\right]$
         2. $\nabla_{\theta^{\prime}} J\left(\theta^{\prime}\right) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \frac{\pi_{\theta^{\prime}}\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)}{\pi_{\theta}\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\mathbf{a}_{i, t} \mid \mathbf{s}_{i, t}\right) \hat{Q}_{i, t}$

3. More source:

   1.  [Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347.pdf)
   2.  [Trust Region Policy Optimization](https://arxiv.org/pdf/1502.05477.pdf)
   3.  [Guided Policy Search](https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf)
   4.  [Deterministic Policy Gradient Algorithms](http://proceedings.mlr.press/v32/silver14.pdf)
   5.  [MAXIMUM A POSTERIORI POLICY OPTIMISATION](https://arxiv.org/pdf/1806.06920.pdf) 