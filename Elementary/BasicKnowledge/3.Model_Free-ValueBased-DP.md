# Value-Based RL
## MDPs
### Markov Decision Process

1. First, we want to formalise the RL interface: agent - environment interaction. For simplicity, we assume the environment is fully observable.

   1. A **Markov Decision Process** is a tuple $(\mathcal{S}, \mathcal{A}, p, \gamma)$, where

      - $\mathcal{S}$ is the set of all possible states
      - $\mathcal{A}$ is the set of all possible actions (e.g., motor controls)
      - $p\left(r, s^{\prime} \mid s, a\right)$ is the joint probability of a reward $r$ and next state $s^{\prime}$, given a state $s$ and action a
      - $\gamma \in[0,1]$ is a discount factor that trades off later rewards to earlier ones

  2. $p$ defines the dynamic of environment : 

     1. **State Transition: ** $p\left(s^{\prime} \mid s, a\right)=\sum_{r} p\left(s^{\prime}, r \mid s, a\right)$
     2. **Expected Reward: **  $\mathbb{E}[R \mid s, a]=\sum_{r} r \sum_{s^{\prime}} p\left(r, s^{\prime} \mid s, a\right)$  

   3. So we can define MDPs by the state transition and expected rewards: 

      1. A **Markov Decision Process is a tuple** $(\mathcal{S}, \mathcal{A}, p, r, \gamma)$, where

         - $\mathcal{S}$ is the set of all possible states

         - $\mathcal{A}$ is the set of all possible actions (e.g., motor controls)

         - $p\left(s^{\prime} \mid s, a\right)$ is the probability of transitioning to $s^{\prime}$, given a state $s$ and action a

         - $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is the excepted reward, achieved on a transition starting in $(s, a)$
           $$
           r=\mathbb{E}[R \mid s, a]
           $$

           

         - $\gamma \in[0,1]$ is a discount factor that trades off later rewards to earlier ones

   4. **Markov Property: ** <u>the future is independent of the past given the present</u>

      1. $p\left(S_{t+1}=s^{\prime} \mid S_{t}=s\right)=p\left(S_{t+1}=s^{\prime} \mid h_{t-1}, S_{t}=s\right)$
      2. which includes all possible histories: $h_{t-1}=\left\{S_{1}, \ldots, S_{t-1}, A_{1}, \ldots, A_{t-1}, R_{1}, \ldots, R_{t-1}\right\}$



## Objective
### Return

1. Rewards: 
   1. immediate rewards: $R_t$
   2. cumulative returns:  $G_t$
2. **Returns**:
   1. Undiscounted Return : $G_{t}=R_{t+1}+R_{t+2}+\ldots+R_{T}=\sum_{k=0}^{T-t-1} R_{t+k+1}$
   2. Discounted Return : $G_{t}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{T-t} R_{T}=\sum_{k=0}^{T-t-1} \gamma^{k} R_{t+k+1}$ 
      1. Why discounted ? return could be extended to **infinite horizon** and **mathematically friendly**
      2. $\gamma$ < 1 : leads to **myopic** and **far-sighted** evaluation 
   3. Average Return : $G_{t}=\frac{1}{T-t-1}\left(R_{t+1}+R_{t+2}+\ldots+R_{T}\right)=\frac{1}{T-t-1} \sum_{k=0}^{T-t-1} R_{t+k+1}$ 

### Policy

1. **Not related to Policy Gradient**. policy gradient is just a method to improve the performance and maximize the return.
2. **Policy could be based on the value function or the policy function**:
   1. A policy is a mapping $\pi: \mathcal{S} \times \mathcal{A} \rightarrow[0,1]$ that, for every state $s$ assigns for each action $a \in \mathcal{A}$ the probability of taking that action in state $s$. Denoted by $\pi(a \mid s)$.

### Value Function

1. **Value Functions**: 

   1. value function: long-term value for a state : $v_{\pi}(s)=\mathbb{E}\left[G_{t} \mid S_{t}=s, \pi\right]$ 
   2. state-action values: $q_{\pi}(s, a)=\mathbb{E}\left[G_{t} \mid S_{t}=s, A_{t}=a, \pi\right]$ 
   3. Connection : $v_{\pi}(s)=\sum_{a} \pi(a \mid s) q_{\pi}(s, a)=\mathbb{E}\left[q_{\pi}\left(S_{t}, A_{t}\right) \mid S_{t}=s, \pi\right], \forall s$ 

2.  **Optimal Value Function: **

   1. First, value function is determined by the policy. Value function could be viewed as an evaluation for the policy.
   2. **Optimal state-value function:  **$v^{*}(s)=\max _{\pi} v_{\pi}(s)$ 
   3. **Optimal action-value function: **  $q^{*}(s, a)=\max _{\pi} q_{\pi}(s, a)$ 
   4. To put it in other words, inaccurate value function leads to inaccurate evaluation for policy and suboptimal policy leads to lower value function.

3. **Optimal Policy** : 

   1. We could define a partial ordering over policy: 

      1. $\pi \geq \pi^{\prime} \Longleftrightarrow v_{\pi}(s) \geq v_{\pi^{\prime}}(s), \forall s$ 
      2. Theorem : Optimal Policies : for all MDPs, there always exists optimal policy.

   2. For example : optimal policy's evaluation -> optimal value function. Optimal value function could also derive the optimal policy, which reveals the connection between policy and value function.

      1. An optimal policy can be found by maximising over $q^{*}(s, a)$,
         $$
         \pi^{*}(s, a)= \begin{cases}1 & \text { if } a=\underset{a \in \mathcal{A}}{\operatorname{argmax}} q^{*}(s, a) \\ 0 & \text { otherwise }\end{cases}
         $$

## Bellman Equations

1. Value Function:  $v_{\pi}(s)=\mathbb{E}\left[G_{t} \mid S_{t}=s, \pi\right]$

   1. Recursive : $\begin{aligned} v_{\pi}(s) &=\mathbb{E}\left[R_{t+1}+\gamma G_{t+1} \mid S_{t}=s, \pi\right] \\ &=\mathbb{E}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s, A_{t} \sim \pi\left(S_{t}\right)\right] \\ &=\sum_{a} \pi(a \mid s) \sum_{r} \sum_{s^{\prime}} p\left(r, s^{\prime} \mid s, a\right)\left(r+\gamma v_{\pi}\left(s^{\prime}\right)\right) \end{aligned}$ 

2. Action values:   $q_{\pi}(s, a)=\mathbb{E}\left[G_{t} \mid S_{t}=s, A_{t}=a, \pi\right]$ 

   1. Recursive :  $\begin{aligned} q_{\pi}(s, a) &=\mathbb{E}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=a\right] \\ &=\mathbb{E}\left[R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, A_{t+1}\right) \mid S_{t}=s, A_{t}=a\right] \\ &=\sum_{r} \sum_{s^{\prime}} p\left(r, s^{\prime} \mid s, a\right)\left(r+\gamma \sum_{a^{\prime}} \pi\left(a^{\prime} \mid s^{\prime}\right) q_{\pi}\left(s^{\prime}, a^{\prime}\right)\right) \end{aligned}$ 

3. Theorem : Bellman Expectation Equation

   1. $$
      \begin{aligned}
      v_{\pi}(s) &=\sum_{a} \pi(s, a)\left[r(s, a)+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid a, s\right) v_{\pi}\left(s^{\prime}\right)\right] \\
      q_{\pi}(s, a) &=r(s, a)+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid a, s\right) \sum_{a^{\prime} \in \mathcal{A}} \pi\left(a^{\prime} \mid s^{\prime}\right) q_{\pi}\left(s^{\prime}, a^{\prime}\right)
      \end{aligned}
      $$

4. Bellman Optimality Equations: every optimal value function will obey : 

   1. $$
      \begin{aligned}
      v^{*}(s) &=\max _{a}\left[r(s, a)+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid a, s\right) v^{*}\left(s^{\prime}\right)\right] \\
      q^{*}(s, a) &=r(s, a)+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid a, s\right) \max _{a^{\prime} \in \mathcal{A}} q^{*}\left(s^{\prime}, a^{\prime}\right)
      \end{aligned}
      $$

## Bellman Equations in Practice

1. Problems in RL according to Bellman Expectation ( Optimality ):

   1. **Policy Evaluation: **given a policy, estimate the expected return under that behaviors. <u>Prediction</u>
   2. **Policy Optimization: ** improve the policy and find the $v_{*}$ or $q_{*}$. <u>Control</u>

2. Toy Example For **Policy Evaluation** : Finite state space, known transition and reward expectation :

   1. Matrix Form:  $\mathbf{v}=\mathbf{r}^{\pi}+\gamma \mathbf{P}^{\pi} \mathbf{v}$

      1. $$
         \begin{aligned}
         v_{i} &=v\left(s_{i}\right) \\
         r_{i}^{\pi} &=\mathbb{E}\left[R_{t+1} \mid S_{t}=s_{i}, A_{t} \sim \pi\left(S_{t}\right)\right] \\
         P_{i j}^{\pi} &=p\left(s_{j} \mid s_{i}\right)=\sum_{a} \pi\left(a \mid s_{i}\right) p\left(s_{j} \mid s_{i}, a\right)
         \end{aligned}
         $$

   2. Solution:  $\begin{aligned} \mathbf{v} &=\mathbf{r}^{\pi}+\gamma \mathbf{P}^{\pi} \mathbf{v} \\\left(\mathbf{I}-\gamma \mathbf{P}^{\pi}\right) \mathbf{v} &=\mathbf{r}^{\pi} \\ \mathbf{v} &=\left(\mathbf{I}-\gamma \mathbf{P}^{\pi}\right)^{-1} \mathbf{r}^{\pi} \end{aligned}$ 

   3. Toy Example: this method's time complexity is $O(S^3)$ , not feasible for large problem

      1. **Dynamic Programming**
      2. **Monte-Carlo evaluation**
      3. **Temporal Difference Learning** 

3. **For Policy Optimization, bellman optimality equation is non-linear. So the matrix solution can not be used**
   1. Can use **Iterative** methods:
      1. Using models / dynamic programming
         - Value iteration
         - Policy iteration
      2. Using samples
        - Monte Carlo
        - Q-learning
        - Sarsa

# Solving Bellman Equation

## Dynamic Programming
### Policy Evaluation

1. Basic idea of dynamic programming: use the bellman expectation equation to turn the **equality** to **update** 
2. Algorithm: (converge when $\gamma < 1$)
   1. First, initialise $v_{0}$, e.g., to zero
   2. Then, iterate
      $\forall s: \quad v_{k+1}(s) \leftarrow \mathbb{E}\left[R_{t+1}+\gamma v_{k}\left(S_{t+1}\right) \mid s, \pi\right]$
   3. Stopping: whenever $v_{k+1}(s)=v_{k}(s)$, for all $s$, we must have found $v_{\pi}$

### Policy Iteratoin

1. **Policy Improvement** : how to use evaluated value function to improve the policy ? 
   1. After evaluating the policy $\pi$ ,  $\forall s: \quad \pi_{\text {new }}(s)=\underset{a}{\operatorname{argmax}} q_{\pi}(s, a)$   
   2. We can have the property: that
      1.  $v_{\pi_{\text {new }}}(s) \geq v_{\pi}(s)$, for all $s$
      2. $q_{\pi_{\text {new }}}(s, a) \geq q_{\pi}(s, a)$

<img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-25 at 10.12.44 AM.png" alt="Screen Shot 2022-01-25 at 10.12.44 AM" style="zoom:50%;" />

2. **Policy Iteration** : have the optimal policy
   1. Policy Evaluation : estimate  $V^{\prime}$
   2. Policy Improvement : Generate $\pi^{\prime} > \pi$ 

### Value Iteration

1. Could we speed up ( combinethe evaluation and improvement ) ?
   1. Usually, we need evaluation between 2 greedy ( improvement )
   2. **Bellman Optimality Equation** : 
      1. $\forall s: \quad v_{k+1}(s) \leftarrow \max _{a} \mathbb{E}\left[R_{t+1}+\gamma v_{k}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=a\right]$ 
2. Algorithms:
   - Initialise $v_{0}$
   - Update: $v_{k+1}(s) \leftarrow \max _{a} \mathbb{E}\left[R_{t+1}+\gamma v_{k}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=s\right]$
   - Stopping: whenever $v_{k+1}(s)=v_{k}(s)$, for all $s$, we must have found $v^{*}$

![Screen Shot 2022-01-25 at 10.19.41 AM](/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-25 at 10.19.41 AM.png)

<img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-25 at 10.20.53 AM.png" alt="Screen Shot 2022-01-25 at 10.20.53 AM" style="zoom:67%;" />

## Asynchronous Dynamic Programming

1. Why **asynchronous DP** ?	
   1. large problem, we may use many agent to interact different part of environment, then they will have overlapped exploration.
   2. **We do not want to update all the states at the same time, asynchronous DP updates parts of state-value at a time** 
   3. Here are 3 **Classic Methods**
2. **In-place DP**
   1. synchronous iteration : each state keep $v_{new}$ and $v_{old}$ , because we need to use old value to calculate expectation
   2. asynchronous iteration: just keep one value, somehow like the difference between off-policy and on-policy
      1. for all $s$ in $\mathcal{S}: \quad v(s) \leftarrow \max _{a} \mathbb{E}\left[R_{t+1}+\gamma v\left(S_{t+1}\right) \mid S_{t}=s\right]$ 
3. **Prioritised Sweeping**
   1. Use the priority queue to guide the state selection : 
      1. bellman error :  $\left|\max _{a} \mathbb{E}\left[R_{t+1}+\gamma v\left(S_{t+1}\right) \mid S_{t}=s\right]-v(s)\right|$ 
      2. Update the bellman error of affected states after each backup.
4. **Real-Time DP** 
   1. Only updates state that are relevant to agent





# Bellman Equation: Theoretical Fundamentals
## Preliminaries
### Normed Vector Space

1. Normed Vector Space : **Vector Space X **  + **a norm  || . ||** 
   1. $\|x\| \geq 0, \forall x \in \mathcal{X}$ and if $\|x\|=0$ then $x=\mathbf{0}$.
   2. $\|\alpha x\|=|\alpha|\|x\|$ (homogeneity)
   3. $\left\|x_{1}+x_{2}\right\| \leq\left\|x_{1}\right\|+\left\|x_{2}\right\|$ (triangle inequality) 

### Contraction Mapping

1. $\alpha$ - **contraction mapping**: An mapping $\mathcal{T}: \mathcal{X} \rightarrow \mathcal{X}$
   1. $\exist \alpha \in [0,1)$, $\left\|\mathcal{T} x_{1}-\mathcal{T}_{x_{2}}\right\| \leq \alpha\left\|x_{1}-x_{2}\right\|$ 
   2. also **Lipschitz** and **continuous** 

### Banach Fixed Point Theorem

1. Theorem Banach Fixed Point Theorem:

   1. Condition:

      1.  Let $\mathcal{X}$ a complete normed vector space, equipped with a norm $\|.\|$ 
      2. $\mathcal{T}: \mathcal{X} \rightarrow \mathcal{X}$ a $\gamma$-contraction mapping,

   2. Results: 

      1. $\mathcal{T}$ has a unique fixed point $x \in \mathcal{X}: \exists ! x^{*} \in \mathcal{X}$ s.t. $\mathcal{T} x^{*}=x^{*}$

      2. $\forall x_{0} \in \mathcal{X}$, the sequence $x_{n+1}=\mathcal{T} x_{n}$ converges to $x^{*}$ in a geometric fashion:
         $$
         \left\|x_{n}-x^{*}\right\| \leq \gamma^{n}\left\|x_{0}-x^{*}\right\|
         $$
         Thus $\lim _{n \rightarrow \infty}\left\|x_{n}-x^{*}\right\| \leq \lim _{n \rightarrow \infty}\left(\gamma^{n}\left\|x_{0}-x^{*}\right\|\right)=0 .$ 

## Bellman Operators
### Bellman Optimality Operator

1. For an MDP, we could define the value function over the state, and the bellman operators are defined over the **value function**. So we actually precise the **Function Space** as the Normed Vector Space.  **Operators are imposed on value function**
2. Bellman Optimality operator $T_{\mathcal{V}}^{*}: \mathcal{V} \rightarrow \mathcal{V}$ ,  $\mathcal{V}$ is rea-valued functions over S 
   1. $\left(T_{\mathcal{V}}^{*} f\right)(s)=\max _{a}\left[r(s, a)+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid a, s\right) f\left(s^{\prime}\right)\right], \forall f \in \mathcal{V}$
3. Properties of Bellman Operator
   1. It has one unique fixed point $v^{*}$.

$$
T^{*} v^{*}=v^{*}
$$
​        2. $T^{*}$ is a $\gamma$-contraction wrt. to $\|\cdot\|_{\infty}$
$$
\left\|T^{*} v-T^{*} u\right\|_{\infty} \leq \gamma\|v-u\|_{\infty}, \forall u, v \in \mathcal{V}
$$
​       3. $T^{*}$ is monotonic:
$$
\forall u, v \in \mathcal{V} s.t. u \leq v, component-wise,\ then\  T^{*} u \leq T^{*} v
$$



### Bellman Expectation Operator

1. Same Idea as optimality operator : 
   1. **Bellman Expectation Operator: ** $\left(T_{\mathcal{V}}^{\pi} f\right)(s)=\sum_{a} \pi(s, a)\left[r(s, a)+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid a, s\right) f\left(s^{\prime}\right)\right], \forall f \in \mathcal{V}$
   2. Same Property as optimality operator:
      1. unique fixed point ; $\gamma$ - contraction ; monotonic

### Bellman for action-state

1. Action-state function: $q^{\pi}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$
   1. **Bellman Expectation Operator** :  $\left(T_{\mathcal{Q}}^{\pi} f\right)(s, a)=r(s, a)+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid a, s\right) \sum_{a^{\prime} \in \mathcal{A}} \pi\left(a^{\prime} \mid s^{\prime}\right) f\left(s^{\prime}, a^{\prime}\right), \forall f \in \mathcal{Q}$
   2. **Bellman Optimality Operator** :  $\left(T_{\mathcal{Q}}^{*} f\right)(s, a)=r(s, a)+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid a, s\right) \max _{a^{\prime} \in \mathcal{A}} f\left(s^{\prime}, a^{\prime}\right), \forall f \in \mathcal{Q}$

## Approximate Bellman Operators

1. Source of Problem:
   1. **Sampling Error: ** <u>Don't know the underlying MDP, that is to say we do not have exact Bellman Operator</u>
      1. for example : continuous state, action space, so we can not calculate the expectation, and we can only sample to estimate
   2.  **Approximation Error: ** <u>Representation of value function is not accurate</u>
      1. for example : bad neural network
