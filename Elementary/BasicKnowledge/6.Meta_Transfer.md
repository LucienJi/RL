# Transfer
## Limit of classic RL
1. **No prior understanding of problem structure**:
   1. For example, *montezuma's revenge* is difficult for agent, because we understand the meaning of *ladders,skull*

2. **No way to store prior tasks**
   1. what kind of knowledge should we store ? Policy ? Value ? Model ? Representation ?
      1. **Representation: ** decouple the **representation layer** and **policy layer**. We remain representaion and destroy the policy to see the recovery time (compared with original training).



## Introduction to Transfer learning for RL
### Terminology
1. **Transfer Learning**: Using **Experience from one set of tasks** for faster learning and better performance **on a new task**
1. **Source Domain**: training set of tasks, for RL, new task is a new MDP
1. **Target Domain** : unseen MDP to be tested
1. **Shot** : number of attempts in the target domain.

### Taxonomy 

1. **Forward transfer**: train on **one task**, transfer to **a new task**
a) Transferring <u>visual representations</u> \& <u>domain adaptation</u>
b) Domain adaptation in reinforcement learning
c) Randomization
2. **Multi-task transfer:** train on many tasks, transfer to a new task
a) <u>Sharing representations</u> and layers across tasks in multi-task learning
b) <u>Contextual</u> policies
c) Optimization challenges for multi-task learning
d) Algorithms
3. **Transferring models and value functions**
a) <u>Model-based RL</u> as a mechanism for transfer
b) Successor features \& representations



## Forward Transfer
### Pretraining + Finetuning

0. Most Popular Way in <u>supervised learning</u> . 
   1. Eg : Classifier trained on imagenet could be used for other tasks like detection.
   2. Why could this work ? :
      1. Shared Representation across different tasks : visual representation.
      2. Close to new task's local optimal, so it could be quickly finetuned.
1. **Problem for RL in pretraining and finetuning**
   1. **Domain Shift **:  
      1. Eg: visual learned representation may not work
   2. **Difference in MDP **: 
      1. Eg: some actions are not possible
   3. **Finetuning issues**
      1. Eg: Big issue for fine-tuning -> FineTuning ususally means insufficient exploration.
      1. **features in RL are less general - task specific** 
      1. **optimal policy in fully observed MDPS are deterministic, less exploration if we begin with the optimal policy in another environment**. 
1. Some Idea:
   1. Pretrain with **robustness** : like solving one task by different policy
   2. Finetune with maximum-entropy policy and slowly converge.
   3. Finetuning via MaxEnt RL: Haarnoja*, Tang*, et al. (2017). Reinforcement Learning with Deep Energy-Based Policies.
   4. Andreas et al. Modular multitask reinforcement learning with policy sketches. $2017 .$
   5. Florensa et al. Stochastic neural networks for hierarchical reinforcement learning. $2017 .$
   6. Kumar et al. One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL. 2020
   

### Domain Adaptation

0. Basic Assumption for Computer Vision: **invariance assumption**: everything that is different between domains is irrelevant.  
1. Invariance are not enough when the dynamics don't match.
2. Suggested Reads:
   1. Tzeng, Hoffman, Zhang, Saenko, Darrell. Deep Domain Confusion: Maximizing for Domain Invariance. $2014 .$
   2. Ganin, Ustinova, Ajakan, Germain, Larochelle, Laviolette, Marchand, Lempitsky. DomainAdversarial Training of Neural Networks. $2015 .$
   3. Tzeng*, Devin*, et al., Adapting Visuomotor Representations with Weak Pairwise Constraints. $2016 .$
   4. Eysenbach et al., Off-Dynamics Reinforcement Learning: Training for Transfer with Domain Classifiers. $2020 .$


### Randomization

1. Design more complex and difficult source domain 
2. Suggested Reading: 
   1. Rajeswaran, et al. (2017). EPOpt: Learning Robust Neural Network Policies Using Model Ensembles.
   2. Yu et al. (2017). Preparing for the Unknown: Learning a Universal Policy with Online System Identification.
   3. Sadeghi \& Levine. (2017). CAD2RL: Real Single Image Flight without a Single Real Image.
   4. Tobin et al. (2017). Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World.
   5. James et al. (2017). Transferring End-to-End Visuomotor Control from Simulation to Real World for a Multi-Stage Task.
   6. Methods that also incorporate domain adaptation together with randomization:
      1. Bousmalis et al. (2017). Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping.
      2. Rao et al. (2017). RL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real.

## Multi-Task Transfer

0. Basic Assumption: learning different tasks could help learn faster
   1. Difficulty in learning multiple tasks at the same time : 
      1. Gradient interference : interfered optimal
      2. Winner-take-all problem : one task return higher average reward, so this task is prioritized

1. Some ideas: 
   1. learn one policy for all games
   2. combine weak policy into strong policy
   3. **contextual policy** 
2. Suggested Reading:
   1. Policy Distillation 
   2. ACTOR-MIMIC DEEP MULTITASK AND TRANSFER REINFORCEMENT LEARNING
   3. Divide and Conquer Reinforcement Learning 

## Transferring Models and Value Functions

0. Basic Assumption: dynamic is the same across domains and reward(goal) may be different. 
   1. For example : 
      1. autonomous car : from a few destination to navigation
      2. kitchen robot: from many recipes to new recipes
   2. **Model is independent of the reward, easily transferred.**
   3. **Value Function entangles the reward and dynamic, may be decomposed ? since value evaluation is linear in reward (if policy and dynamic remain the same)** 
   4. But policy contains least dynamic info



# Meta-Learning

## Introduction

1. What is meta-learning ?
   1. Meta-Learning : (abstract) learning to learn
   2. In practice : close to multi-task
   3. Different formulation : learn an optimizer ; learn an RNN which encode experience; learn a representation
2. Why Meta-learning ?
   1. Model Free requires a lot of samples. First meta-learn a faster learner, then learn new tasks quickly.
   2. **RL Meta-Learner in new environments**: 
      1. **He knows how to explore more intelligently**.
      2. **Avoid trying to actions that are known to be useless**. 
      3. **Learn an representation of new tasks quickly**. 
   3. Supervised Meta Learning : 
      1. Classic Learning: one input -> one label
      2. Meta Learning : whole training set -> one label  (classifier could retain the information in training set) 
      3. RNN-Style Meta Learning : 
         1. $\theta^{\star}=\arg \min _{\theta} \sum_{i=1}^{n} \mathcal{L}\left(\phi_{i}, \mathcal{D}_{i}^{\mathrm{ts}}\right)$ , where $\phi_{i}=f_{\theta}\left(\mathcal{D}_{i}^{\mathrm{tr}}\right)$.   $\phi_{i}=\left[h_{i}, \theta_{p}\right]$  , where h is the hidden state

## Meta RL 

<img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-02-03 at 2.36.01 PM.png" alt="Screen Shot 2022-02-03 at 2.36.01 PM" style="zoom: 50%;" /> 

1. Problem Setting : 
   1. Many Tasks (MDP) : $\mathcal{M}_{i} \sim p(\mathcal{M})$ 
   2. Goal : Learn a meta policy -> $\theta^{\star}=\arg \max _{\theta} \sum_{i=1}^{n} E_{\pi_{\phi_{i}}(\tau)}[R(\tau)]$ 
      1. policy is controlled by the $\phi_{i}$ 
      2. $\phi$ is defined (inferred) from the MDP $\phi_{i}=f_{\theta}\left(\mathcal{M}_{i}\right)$ 

2. Current 3 Perspective on Meta-RL
   1. Just RNN for Meta : I could remember all
      1. Good : conceptually simple; easy to try
      2. Bad : meta-overfitting; not easy to optimize
   2. Bi-Level Optimization(MAML) : 
      1. Good : good extrapolation; conceptually elegant
      2. Bad : complex + need more samples 
   3. POMDP (Bayesian RL) : 
      1. Good : could be simple and effective ; could guide exploration
      2. Bad : same problem as RNN (most Bayesian RL )

### Recurrent and Contextual Policy

1. Use the history to infer the $\phi$ :  $\pi_{\theta}\left(a_{t} \mid s_{t}, s_{1}, a_{1}, r_{1}, \ldots, s_{t-1}, a_{t-1}, r_{t-1}\right)$ 
   1. Maybe **an RNN policy is sufficient ? Not reset between episodes** . $\phi_i$ is deduced from hidden state and meta-learner's parameters. 
      <img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-02-03 at 2.48.43 PM.png" alt="Screen Shot 2022-02-03 at 2.48.43 PM" style="zoom: 50%;" />


2. Suggested Reading 

   1. Heess, Hunt, Lillicrap, Silver. Memory-based control with recurrent neural networks. $2015 .$
   2. Wang, Kurth-Nelson, Tirumala, Soyer, Leibo, Munos, Blundell, Kumaran, Botvinick. Learning to Reinforcement Learning. $2016 .$
   3. Duan, Schulman, Chen, Bartlett, Sutskever, Abbeel. RL2: Fast Reinforcement Learning via Slow Reinforcement Learning. $2016 .$ 
   3. Ritter, Wang, Kurth-Nelson, Jayakumar, Blundell, Pascanu, Botvinick. Been There, Done That: Meta-Learning with Episodic Recall. 
   3. Wang, Kurth-Nelson, Kumaran, Tirumala, Soyer, Leibo, Hassabis, Botvinick. Prefrontal Cortex as a MetaReinforcement Learning System. 
   3. Dasgupta, Wang, Chiappa, Mitrovic, Ortega, Raposo, Hughes, Battaglia, Botvinick, Kurth-Nelson. Causal Reasoning from Meta-Reinforcement Learning. 



### Architectures For Meta-RL
1. Architectures :
	1. Standard RNN
	2. Attention + Temporal convolution
	3. Parallel Permutation-invariant context encoder
1. Suggested Reading:
	1. Mishra, Rohaninejad, Chen, Abbeel. A Simple Neural Attentive Meta-Learner.
	2. Rakelly*, Zhou*, Quillen, Finn, Levine. Efficient Off-Policy MetaReinforcement learning via Probabilistic Context Variables.
	3. Duan, Schulman, Chen, Bartlett, Sutskever, Abbeel. RL2: Fast Reinforcement Learning via Slow Reinforcement Learning. $2016 .$ 
	
### Gradient-Based Meta Learning

1. Consider the Meta RL as an optimization problem -> consider $f_{\theta}(M_i)$ as RL algo 
   1. MAML RL : 
      1. $f_{\mathrm{MAML}}\left(\mathcal{D}^{\mathrm{tr}}, x\right)=f_{\theta^{\prime}}(x)$
      2. $\theta^{\prime}=\theta-\alpha \sum_{(x, y) \in \mathcal{D}^{\operatorname{tr}}} \nabla_{\theta} \mathcal{L}\left(f_{\theta}(x), y\right)$

2. Suggested Reading: 
   1. **MAML meta-policy gradient estimators**:
      1. Finn, Abbeel, Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.
      2. Foerster, Farquhar, Al-Shedivat, Rocktaschel, Xing, Whiteson. DiCE: The Infinitely Differentiable Monte Carlo Estimator.
      3. Rothfuss, Lee, Clavera, Asfour, Abbeel. ProMP: Proximal Meta-Policy Search.
   2. **Improving exploration**:
      1. Gupta, Mendonca, Liu, Abbeel, Levine. Meta-Reinforcement Learning of Structured Exploration Strategies.
      2. Stadie*, Yang*, Houthooft, Chen, Duan, Wu, Abbeel, Sutskever. Some Considerations on Learning to Explore via Meta-Reinforcement Learning.
   3. **Hybrid algorithms (not necessarily gradient-based)**:
      1. Houthooft, Chen, Isola, Stadie, Wolski, Ho, Abbeel. Evolved Policy Gradients.
      2. Fernando, Sygnowski, Osindero, Wang, Schaul, Teplyashin, Sprechmann, Pirtzel, Rusu. MetaLearning by the Baldwin Effect. 

### Meta RL as POMDP

1. Basic Assumption : 
   1. Solving the POMDP is equivalent to meta-learning 
   2. Learn an extended state to encapsulate information

2. Some idea:
   1. **Learning a task = inferring z** from the context ; explore via posterior sampling with latent context
      1. sample : $z \sim \hat{p}\left(z_{t} \mid s_{1: t}, a_{1: t}, r_{1: t}\right)$ 
      2. act : according to $\pi_{\theta}(a \mid s, z)$ to collect more data 
   2. **Variational Inference for Meta-RL** 
      1. the aforementioned latent z is to represent a task. Here variational inference to encode the z close to prior. Thus we could use posterior sampling to guide the exploration (use different z to have different behavior)



2. Suggested Reading
   1. See, e.g. Russo, Roy. Learning to Optimize via Posterior Sampling. 
   2. Rakelly*, Zhou*, Quillen, Finn, Levine. Efficient Off-Policy Meta-Reinforcement learning via Probabilistic Context Variables. ICML $2019 .$ 
   3. Zhao, Nagabandi, Rakelly, Finn, Levine. MELD: Meta-Reinforcement Learning from Images via Latent State Models. '20 
   4. Zintgraf, Igl, Shiarlis, Mahajan, Hofmann, Whiteson. Variational Task Embeddings for Fast Adaptation in Deep Reinforcement Learning.
   - Humplik, Galashov, Hasenclever, Ortega, Teh, Heess. Meta reinforcement learning as task inference.



## Model-Based Meta RL

==Good Direction for Robots, to be continued==

0. Suggested Reading:

   1. Saemundsson, Hofmann, Deisenroth. Meta-Reinforcement Learning with Latent Variable Gaussian Processes. 

   2. Nagabandi, Finn, Levine. Deep Online Learning via MetaLearning: Continual Adaptation for Model-Based RL.

   3. Nagabandi*, Clavera*, Liu, Fearing, Abbeel, Levine, Finn. Learning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning. ICLR $2019 .$ 

   4. Model-Based Meta-Reinforcement Learning for Flight with Suspended Payloads. 

      
