# Model-Based RL
0. Compare with other RL
   1. Dynamic Programming : know the model completely and **solve** it
   2. Model-Free RL : no model and **learn** the value function from experience  
   3. **Model-Based RL** : learn a model from experience and plan value functions using the learned model. 
1. Why and why not RL ?
   1. models could be learned efficiently with supervised learning
   2. we could reason about the model uncertainty -> better exploration
   3. reduce the interaction with the real world -> data efficiency



## Model Learning
1. What is a model ?
   1. **approximate** representation of an MDP $\langle\mathcal{S}, \mathcal{A}, \hat{p}\rangle$ 
   2. approximates the **state transitions and rewards** $\hat{p}_{\eta} \approx p:$  $R_{t+1}, S_{t+1} \sim \hat{p}_{\eta}\left(r, s^{\prime} \mid S_{t}, A_{t}\right)$ 
   3. how could we learn a function : $f_{\eta}(s, a)=r, s^{\prime}$ ?
2. Different examples of Models : 
   1. Transition dynamics
   2. Reward dynamics
3. Different Options : 
   1. Table Lookup model
   2. Linear Expectation Model
   3. Deep Neural Network Model

### Table Lookup Model
1. Explicit MDP with Count Visit $N(s,a)$:
   1. Dynamic Transition :  $\hat{p}_{t}\left(s^{\prime} \mid s, a\right)=\frac{1}{N(s, a)} \sum_{k=0}^{t-1} I\left(S_{k}=s, A_{k}=a, S_{k+1}=s^{\prime}\right)$
   2. Reward Transition :  $\mathbb{E}_{\hat{p}_{t}}\left[R_{t+1} \mid S_{t}=s, A_{t}=a\right]=\frac{1}{N(s, a)} \sum_{k=0}^{t-1} I\left(S_{k}=s, A_{k}=a\right) R_{k+1}$ 

### Linear Expectation Model

1. Linear Expectation Model:
   1. Feature Representation $\phi$ , each state could be encoded as $\phi(s)$ 
   2. Parmetrise separately rewards and transitions -> each as a **linear model of features** 
2. **Linear Model**
   1. Expected next states based on the current state and action could be modeled as the **Product of square matrix (s,a) and current state feature** : 
      1. $\hat{s}^{\prime}(s, a)=T_{a} \phi(s)$, where $T_a$  is a square matrix for each state and action
   2. Reward could be modeled as **Product of vector of (s,a) and current state feature** 
      1. $\hat{r}(s, a)=w_{a}^{T} \phi(s)$ 
   3. Minimize the loss : 
      1. $L\left(s, a, r, s^{\prime}\right)=\left(s^{\prime}-T_{a} \phi(s)\right)^{2}+\left(r-w_{a}^{T} \phi(s)\right)^{2}$ 



## Planning with Model

### Dynamic Programming with Learned Model
1. Basic Idea : Solve the approximate MDP $\left\langle\mathcal{S}, \mathcal{A}, \hat{p}_{\eta}\right\rangle$  to learn **optimal value function**
	1. Use the previous DP algo
2. Limits : even though you could solve the optimal algorithm, you may compute the **suboptimal** policy according to the approximate 

### Sample-Based Planning with Learned Model
1. Basic Idea : use the model to **generate samples**  and then **apply model-free** RL algo **to learn value function**
   1. MC, TD to $S, R \sim \hat{p}_{\eta}(\cdot \mid s, a)$ 

### Simulation-Based Planning with Learned Model

1. Basic Idea : Use simulation to help **action selection**
   1. For example, if we use greedy policy based on action-value function, we could use simulation to better evaluate the action-value function. 
2. **Forward Search** : build a search tree
   1. **Monte-Carlo Simulation** 
      1. Use the **Parameterized model M ** and a **simulation policy** $\pi$ 
      2. Evaluate state by mean return : $v\left(S_{t}\right)=\frac{1}{K} \sum_{k=1}^{K} G_{t}^{k} \rightsquigarrow v_{\pi}\left(S_{t}\right)$  ;  $q(s, a)=\frac{1}{K} \sum_{k=1}^{K} G_{t}^{k} \rightsquigarrow q_{\pi}(s, a)$ 
      3. More specifically: select, expand, rollout and update. 
   2. **Advantages of Simulation** :
      1. unlike DP , we may just build part of model not like the DP which needs a whole model.
      2. use sampling to break the curse of dimensionality 
      3. works for the black-box models 
      4. we could consider the search tree as a **partial** **instantiation** of the lookup table.  Tree stores the values for easily reachable states. 

## Integrating Learning and Planning
### Dyna 

<img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-30 at 9.01.48 PM.png" alt="Screen Shot 2022-01-30 at 9.01.48 PM" style="zoom:50%;" />

0. Basic Idea : 

   1. learn a model from real experience 
   2. Learn and plan value function from **Real** and **Simulated** experience
   3. Updates from learning and planning are not distinguished 

1. Dyna-Q Algo : 

   1. Initialize $Q(s, a)$ and $\operatorname{Model}(s, a)$ for all $s \in \mathcal{S}$ and $a \in \mathcal{A}(s)$
      1. Do forever:
         	(a) $s \leftarrow$ current (nonterminal) state
         	(b) $a \leftarrow \varepsilon$-greedy $(s, Q)$
         	(c) Execute action $a ;$ observe resultant state, $s^{\prime}$, and reward, $r$
         	(d) $Q(s, a) \leftarrow Q(s, a)+\alpha\left[r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)\right]$
         	(e) $M o d e l(s, a) \leftarrow s^{\prime}, r \quad$ (assuming deterministic environment)
         	(f) Repeat $N$ times:

   $$
   s \leftarrow \operatorname{random} \text { previously observed state }\\
   a \leftarrow \operatorname{random}\ action\ previously\ taken\ in\ s\\
   s^{\prime}, r \leftarrow \operatorname{Model}(s, a)\\
   Q(s, a) \leftarrow Q(s, a)+\alpha\left[r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)\right]\\
   $$
   

### Compare Learned Model and Experience Replay

1. What's the difference between learned model and experience replay ?
   1. In dyna, we use model and replay buffer for training. If the model is perfect, model's inference is equivalent tp replay buffer but with more flexibility 
   2. In practice, **model is not perfect**,  But Learned Model is more flexible :
      1. Plan for action-selection
      2. Couterfactual planning : **could have done** 
      3. querying a replay buffer is cheap but needs the memory capacity 





## Planning with Derivatives



0. Basic Idea 
   1. Shooting methods:
      1. $\min _{\mathbf{u}_{1}, \ldots, \mathbf{u}_{T}} c\left(\mathbf{x}_{1}, \mathbf{u}_{1}\right)+c\left(f\left(\mathbf{x}_{1}, \mathbf{u}_{1}\right), \mathbf{u}_{2}\right)+\cdots+c\left(f(f(\ldots) \ldots), \mathbf{u}_{T}\right)$ 
      2. Only optimize over actions 
   2. Collocation : 
      1. $\min _{\mathbf{u}_{1}, \ldots, \mathbf{u}_{T}, \mathbf{x}_{1}, \ldots, \mathbf{x}_{T}} \sum_{t=1}^{T} c\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)$ s.t. $\mathbf{x}_{t}=f\left(\mathbf{x}_{t-1}, \mathbf{u}_{t-1}\right)$
      2. optimize over actions and states with constraints 

### Linear Case : LQR

1. Assumption : Linear Transition + Quadratic Cost
   1. $f\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)=\mathbf{F}_{t}\left[\begin{array}{l}\mathbf{x}_{t} \\ \mathbf{u}_{t}\end{array}\right]+\mathbf{f}_{t}$ 
   2. $c\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)=\frac{1}{2}\left[\begin{array}{l}\mathbf{x}_{t} \\ \mathbf{u}_{t}\end{array}\right]^{T} \mathbf{C}_{t}\left[\begin{array}{l}\mathbf{x}_{t} \\ \mathbf{u}_{t}\end{array}\right]+\left[\begin{array}{l}\mathbf{x}_{t} \\ \mathbf{u}_{t}\end{array}\right]^{T} \mathbf{c}_{t}$ 
2. Backward Recursion : LQR 

### Stochastic Dynamics 

1. Assumption : stochastic dynamic 
   1. $f\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)=\mathbf{F}_{t}\left[\begin{array}{l}\mathbf{x}_{t} \\ \mathbf{u}_{t}\end{array}\right]+\mathbf{f}_{t}$ 
   2. $p\left(\mathbf{x}_{t+1} \mid \mathbf{x}_{t}, \mathbf{u}_{t}\right)=\mathcal{N}\left(\mathbf{F}_{t}\left[\begin{array}{l}\mathbf{x}_{t} \\ \mathbf{u}_{t}\end{array}\right]+\mathbf{f}_{t}, \Sigma_{t}\right)$ 
2. No change for the algorithm 



### Nonlinear Case : DDP/Iterative LQR

1. Approximate a nonlinear system as a linear-quadratic system : 

   1. $f\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right) \approx f\left(\hat{\mathbf{x}}_{t}, \hat{\mathbf{u}}_{t}\right)+\nabla_{\mathbf{x}_{t}, \mathbf{u}_{t}} f\left(\hat{\mathbf{x}}_{t}, \hat{\mathbf{u}}_{t}\right)\left[\begin{array}{l}\mathbf{x}_{t}-\hat{\mathbf{x}}_{t} \\ \mathbf{u}_{t}-\hat{\mathbf{u}}_{t}\end{array}\right]$

   2. $c\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right) \approx c\left(\hat{\mathbf{x}}_{t}, \hat{\mathbf{u}}_{t}\right)+\nabla_{\mathbf{x}_{t}, \mathbf{u}_{t}} c\left(\hat{\mathbf{x}}_{t}, \hat{\mathbf{u}}_{t}\right)\left[\begin{array}{l}\mathbf{x}_{t}-\hat{\mathbf{x}}_{t} \\ \mathbf{u}_{t}-\hat{\mathbf{u}}_{t}\end{array}\right]+\frac{1}{2}\left[\begin{array}{l}\mathbf{x}_{t}-\hat{\mathbf{x}}_{t} \\ \mathbf{u}_{t}-\hat{\mathbf{u}}_{t}\end{array}\right]^{T} \nabla_{\mathbf{x}_{t}, \mathbf{u}_{t}}^{2} c\left(\hat{\mathbf{x}}_{t}, \hat{\mathbf{u}}_{t}\right)\left[\begin{array}{l}\mathbf{x}_{t}-\hat{\mathbf{x}}_{t} \\ \mathbf{u}_{t}-\hat{\mathbf{u}}_{t}\end{array}\right]$ 

   3. Look at the dynamic of transition : 

      $\bar{f}\left(\delta \mathbf{x}_{t}, \delta \mathbf{u}_{t}\right)=\underbrace{\mathbf{F}_{t}}_{\downarrow}\left[\begin{array}{l}\delta \mathbf{x}_{t} \\ \delta \mathbf{u}_{t}\end{array}\right]$
      $\nabla_{\mathbf{x}_{t}, \mathbf{u}_{t}} f\left(\hat{\mathbf{x}}_{t}, \hat{\mathbf{u}}_{t}\right)$ 

   4. Approximate dynamic cost : 

      

<img src="/Users/jijingtian/Library/Application Support/typora-user-images/Screen Shot 2022-01-31 at 3.52.33 PM.png" alt="Screen Shot 2022-01-31 at 3.52.33 PM" style="zoom: 50%;" />

2. Algorithm:
   1. iLQR : approximation of Newton'method
   2. Differential dynamic programming (DDP) 